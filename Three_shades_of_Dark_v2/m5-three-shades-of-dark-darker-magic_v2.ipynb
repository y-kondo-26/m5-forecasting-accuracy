{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=514+915+127):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "    \n",
    "    # make snap\n",
    "    if 'CA' in store:\n",
    "        df['snap']=df['snap_CA']\n",
    "    elif 'TX' in store:\n",
    "        df['snap']=df['snap_TX']\n",
    "    elif 'WI' in store:\n",
    "        df['snap']=df['snap_WI']\n",
    "    \n",
    "    df = df.drop(['snap_CA','snap_TX','snap_WI'], axis=1)\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "       \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "# 'max_bin': 100\n",
    "\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514\n",
    "# not our case but good to know cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES=72\n"
     ]
    }
   ],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 2                          # Our model version\n",
    "SEED = 514+915+127                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f'N_CORES={N_CORES}')\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = './'\n",
    "BASE     = './grid_part_1.pkl.gz'\n",
    "PRICE    = './grid_part_2.pkl.gz'\n",
    "CALENDAR = './grid_part_3.pkl.gz'\n",
    "LAGS     = './lags_df_28.pkl.gz'\n",
    "MEAN_ENC = './mean_encoding_df.pkl.gz'\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = './m5-aux-models/'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv.gz')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14,336]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Aux Models\n",
    "# If you don't want to wait hours and hours\n",
    "# to have result you can train each store \n",
    "# in separate kernel and then just join result.\n",
    "\n",
    "# If we want to use pretrained models we can \n",
    "## skip training \n",
    "## (in our case do dummy training\n",
    "##  to show that we are good with memory\n",
    "##  and you can safely use this (all kernel) code)\n",
    "if USE_AUX:\n",
    "    lgb_params['n_estimators'] = 2\n",
    "    \n",
    "# Here is some 'logs' that can compare\n",
    "#Train CA_1\n",
    "#[100]\tvalid_0's rmse: 2.02289\n",
    "#[200]\tvalid_0's rmse: 2.0017\n",
    "#[300]\tvalid_0's rmse: 1.99239\n",
    "#[400]\tvalid_0's rmse: 1.98471\n",
    "#[500]\tvalid_0's rmse: 1.97923\n",
    "#[600]\tvalid_0's rmse: 1.97284\n",
    "#[700]\tvalid_0's rmse: 1.96763\n",
    "#[800]\tvalid_0's rmse: 1.9624\n",
    "#[900]\tvalid_0's rmse: 1.95673\n",
    "#[1000]\tvalid_0's rmse: 1.95201\n",
    "#[1100]\tvalid_0's rmse: 1.9476\n",
    "#[1200]\tvalid_0's rmse: 1.9434\n",
    "#[1300]\tvalid_0's rmse: 1.9392\n",
    "#[1400]\tvalid_0's rmse: 1.93446\n",
    "\n",
    "#Train CA_2\n",
    "#[100]\tvalid_0's rmse: 1.88949\n",
    "#[200]\tvalid_0's rmse: 1.84767\n",
    "#[300]\tvalid_0's rmse: 1.83653\n",
    "#[400]\tvalid_0's rmse: 1.82909\n",
    "#[500]\tvalid_0's rmse: 1.82265\n",
    "#[600]\tvalid_0's rmse: 1.81725\n",
    "#[700]\tvalid_0's rmse: 1.81252\n",
    "#[800]\tvalid_0's rmse: 1.80736\n",
    "#[900]\tvalid_0's rmse: 1.80242\n",
    "#[1000]\tvalid_0's rmse: 1.79821\n",
    "#[1100]\tvalid_0's rmse: 1.794\n",
    "#[1200]\tvalid_0's rmse: 1.78973\n",
    "#[1300]\tvalid_0's rmse: 1.78552\n",
    "#[1400]\tvalid_0's rmse: 1.78158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "[100]\tvalid_0's rmse: 2.02104\n",
      "[200]\tvalid_0's rmse: 2.00183\n",
      "[300]\tvalid_0's rmse: 1.9924\n",
      "[400]\tvalid_0's rmse: 1.98545\n",
      "[500]\tvalid_0's rmse: 1.97846\n",
      "[600]\tvalid_0's rmse: 1.97312\n",
      "[700]\tvalid_0's rmse: 1.96754\n",
      "[800]\tvalid_0's rmse: 1.96268\n",
      "[900]\tvalid_0's rmse: 1.95733\n",
      "[1000]\tvalid_0's rmse: 1.95207\n",
      "[1100]\tvalid_0's rmse: 1.94733\n",
      "[1200]\tvalid_0's rmse: 1.9433\n",
      "[1300]\tvalid_0's rmse: 1.93869\n",
      "[1400]\tvalid_0's rmse: 1.93443\n",
      "Train CA_2\n",
      "[100]\tvalid_0's rmse: 1.89015\n",
      "[200]\tvalid_0's rmse: 1.84797\n",
      "[300]\tvalid_0's rmse: 1.8365\n",
      "[400]\tvalid_0's rmse: 1.82944\n",
      "[500]\tvalid_0's rmse: 1.82217\n",
      "[600]\tvalid_0's rmse: 1.81589\n",
      "[700]\tvalid_0's rmse: 1.81043\n",
      "[800]\tvalid_0's rmse: 1.80574\n",
      "[900]\tvalid_0's rmse: 1.80053\n",
      "[1000]\tvalid_0's rmse: 1.79579\n",
      "[1100]\tvalid_0's rmse: 1.79087\n",
      "[1200]\tvalid_0's rmse: 1.78626\n",
      "[1300]\tvalid_0's rmse: 1.78205\n",
      "[1400]\tvalid_0's rmse: 1.77773\n",
      "Train CA_3\n",
      "[100]\tvalid_0's rmse: 2.50235\n",
      "[200]\tvalid_0's rmse: 2.45611\n",
      "[300]\tvalid_0's rmse: 2.43419\n",
      "[400]\tvalid_0's rmse: 2.4216\n",
      "[500]\tvalid_0's rmse: 2.41048\n",
      "[600]\tvalid_0's rmse: 2.40401\n",
      "[700]\tvalid_0's rmse: 2.39636\n",
      "[800]\tvalid_0's rmse: 2.39066\n",
      "[900]\tvalid_0's rmse: 2.38478\n",
      "[1000]\tvalid_0's rmse: 2.37948\n",
      "[1100]\tvalid_0's rmse: 2.37456\n",
      "[1200]\tvalid_0's rmse: 2.3695\n",
      "[1300]\tvalid_0's rmse: 2.36482\n",
      "[1400]\tvalid_0's rmse: 2.36072\n",
      "Train CA_4\n",
      "[100]\tvalid_0's rmse: 1.33249\n",
      "[200]\tvalid_0's rmse: 1.3252\n",
      "[300]\tvalid_0's rmse: 1.32004\n",
      "[400]\tvalid_0's rmse: 1.31606\n",
      "[500]\tvalid_0's rmse: 1.31236\n",
      "[600]\tvalid_0's rmse: 1.309\n",
      "[700]\tvalid_0's rmse: 1.30594\n",
      "[800]\tvalid_0's rmse: 1.30287\n",
      "[900]\tvalid_0's rmse: 1.29969\n",
      "[1000]\tvalid_0's rmse: 1.29696\n",
      "[1100]\tvalid_0's rmse: 1.29426\n",
      "[1200]\tvalid_0's rmse: 1.29186\n",
      "[1300]\tvalid_0's rmse: 1.28913\n",
      "[1400]\tvalid_0's rmse: 1.28654\n",
      "Train TX_1\n",
      "[100]\tvalid_0's rmse: 1.60961\n",
      "[200]\tvalid_0's rmse: 1.59039\n",
      "[300]\tvalid_0's rmse: 1.58244\n",
      "[400]\tvalid_0's rmse: 1.57685\n",
      "[500]\tvalid_0's rmse: 1.57253\n",
      "[600]\tvalid_0's rmse: 1.56828\n",
      "[700]\tvalid_0's rmse: 1.56412\n",
      "[800]\tvalid_0's rmse: 1.56036\n",
      "[900]\tvalid_0's rmse: 1.55651\n",
      "[1000]\tvalid_0's rmse: 1.55275\n",
      "[1100]\tvalid_0's rmse: 1.54881\n",
      "[1200]\tvalid_0's rmse: 1.54487\n",
      "[1300]\tvalid_0's rmse: 1.54181\n",
      "[1400]\tvalid_0's rmse: 1.53827\n",
      "Train TX_2\n",
      "[100]\tvalid_0's rmse: 1.71856\n",
      "[200]\tvalid_0's rmse: 1.69571\n",
      "[300]\tvalid_0's rmse: 1.68299\n",
      "[400]\tvalid_0's rmse: 1.67682\n",
      "[500]\tvalid_0's rmse: 1.67218\n",
      "[600]\tvalid_0's rmse: 1.66814\n",
      "[700]\tvalid_0's rmse: 1.66369\n",
      "[800]\tvalid_0's rmse: 1.65926\n",
      "[900]\tvalid_0's rmse: 1.65485\n",
      "[1000]\tvalid_0's rmse: 1.65122\n",
      "[1100]\tvalid_0's rmse: 1.64787\n",
      "[1200]\tvalid_0's rmse: 1.64481\n",
      "[1300]\tvalid_0's rmse: 1.64165\n",
      "[1400]\tvalid_0's rmse: 1.63867\n",
      "Train TX_3\n",
      "[100]\tvalid_0's rmse: 1.69075\n",
      "[200]\tvalid_0's rmse: 1.67702\n",
      "[300]\tvalid_0's rmse: 1.67042\n",
      "[400]\tvalid_0's rmse: 1.6643\n",
      "[500]\tvalid_0's rmse: 1.65915\n",
      "[600]\tvalid_0's rmse: 1.65345\n",
      "[700]\tvalid_0's rmse: 1.64842\n",
      "[800]\tvalid_0's rmse: 1.64373\n",
      "[900]\tvalid_0's rmse: 1.63985\n",
      "[1000]\tvalid_0's rmse: 1.63602\n",
      "[1100]\tvalid_0's rmse: 1.63166\n",
      "[1200]\tvalid_0's rmse: 1.62741\n",
      "[1300]\tvalid_0's rmse: 1.62409\n",
      "[1400]\tvalid_0's rmse: 1.62062\n",
      "Train WI_1\n",
      "[100]\tvalid_0's rmse: 1.59435\n",
      "[200]\tvalid_0's rmse: 1.57553\n",
      "[300]\tvalid_0's rmse: 1.56884\n",
      "[400]\tvalid_0's rmse: 1.56413\n",
      "[500]\tvalid_0's rmse: 1.55898\n",
      "[600]\tvalid_0's rmse: 1.55394\n",
      "[700]\tvalid_0's rmse: 1.5497\n",
      "[800]\tvalid_0's rmse: 1.54575\n",
      "[900]\tvalid_0's rmse: 1.54152\n",
      "[1000]\tvalid_0's rmse: 1.5379\n",
      "[1100]\tvalid_0's rmse: 1.53447\n",
      "[1200]\tvalid_0's rmse: 1.53069\n",
      "[1300]\tvalid_0's rmse: 1.52732\n",
      "[1400]\tvalid_0's rmse: 1.52399\n",
      "Train WI_2\n",
      "[100]\tvalid_0's rmse: 2.68769\n",
      "[200]\tvalid_0's rmse: 2.58487\n",
      "[300]\tvalid_0's rmse: 2.54953\n",
      "[400]\tvalid_0's rmse: 2.52998\n",
      "[500]\tvalid_0's rmse: 2.51332\n",
      "[600]\tvalid_0's rmse: 2.49817\n",
      "[700]\tvalid_0's rmse: 2.4867\n",
      "[800]\tvalid_0's rmse: 2.47357\n",
      "[900]\tvalid_0's rmse: 2.46157\n",
      "[1000]\tvalid_0's rmse: 2.45077\n",
      "[1100]\tvalid_0's rmse: 2.44285\n",
      "[1200]\tvalid_0's rmse: 2.43357\n",
      "[1300]\tvalid_0's rmse: 2.42524\n",
      "[1400]\tvalid_0's rmse: 2.41514\n",
      "Train WI_3\n",
      "[100]\tvalid_0's rmse: 1.91901\n",
      "[200]\tvalid_0's rmse: 1.85205\n",
      "[300]\tvalid_0's rmse: 1.82763\n",
      "[400]\tvalid_0's rmse: 1.81553\n",
      "[500]\tvalid_0's rmse: 1.80581\n",
      "[600]\tvalid_0's rmse: 1.79551\n",
      "[700]\tvalid_0's rmse: 1.78836\n",
      "[800]\tvalid_0's rmse: 1.78258\n",
      "[900]\tvalid_0's rmse: 1.77513\n",
      "[1000]\tvalid_0's rmse: 1.76915\n",
      "[1100]\tvalid_0's rmse: 1.764\n",
      "[1200]\tvalid_0's rmse: 1.75825\n",
      "[1300]\tvalid_0's rmse: 1.75388\n",
      "[1400]\tvalid_0's rmse: 1.74897\n"
     ]
    }
   ],
   "source": [
    "########################### Train Models\n",
    "#################################################################################\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "    train_data.save_binary('train_data.bin')\n",
    "    train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  0.45 min round |  0.45 min total |  36984.03 day sales |\n",
      "Predict | Day: 2\n",
      "##########  0.46 min round |  0.91 min total |  34788.12 day sales |\n",
      "Predict | Day: 3\n",
      "##########  0.46 min round |  1.37 min total |  34255.97 day sales |\n",
      "Predict | Day: 4\n",
      "##########  0.46 min round |  1.83 min total |  34828.50 day sales |\n",
      "Predict | Day: 5\n",
      "##########  0.46 min round |  2.29 min total |  41384.79 day sales |\n",
      "Predict | Day: 6\n",
      "##########  0.45 min round |  2.74 min total |  50754.45 day sales |\n",
      "Predict | Day: 7\n",
      "##########  0.46 min round |  3.20 min total |  52348.61 day sales |\n",
      "Predict | Day: 8\n",
      "##########  0.46 min round |  3.66 min total |  43954.85 day sales |\n",
      "Predict | Day: 9\n",
      "##########  0.46 min round |  4.12 min total |  44013.05 day sales |\n",
      "Predict | Day: 10\n",
      "##########  0.46 min round |  4.58 min total |  38462.78 day sales |\n",
      "Predict | Day: 11\n",
      "##########  0.45 min round |  5.03 min total |  40112.54 day sales |\n",
      "Predict | Day: 12\n",
      "##########  0.46 min round |  5.49 min total |  45554.93 day sales |\n",
      "Predict | Day: 13\n",
      "##########  0.46 min round |  5.95 min total |  53411.77 day sales |\n",
      "Predict | Day: 14\n",
      "##########  0.46 min round |  6.41 min total |  45455.94 day sales |\n",
      "Predict | Day: 15\n",
      "##########  0.46 min round |  6.86 min total |  44395.24 day sales |\n",
      "Predict | Day: 16\n",
      "##########  0.46 min round |  7.32 min total |  38986.10 day sales |\n",
      "Predict | Day: 17\n",
      "##########  0.46 min round |  7.77 min total |  39933.74 day sales |\n",
      "Predict | Day: 18\n",
      "##########  0.46 min round |  8.23 min total |  40494.71 day sales |\n",
      "Predict | Day: 19\n",
      "##########  0.46 min round |  8.69 min total |  43676.54 day sales |\n",
      "Predict | Day: 20\n",
      "##########  0.46 min round |  9.15 min total |  53287.15 day sales |\n",
      "Predict | Day: 21\n",
      "##########  0.46 min round |  9.61 min total |  55356.32 day sales |\n",
      "Predict | Day: 22\n",
      "##########  0.46 min round |  10.06 min total |  41617.35 day sales |\n",
      "Predict | Day: 23\n",
      "##########  0.46 min round |  10.52 min total |  37490.25 day sales |\n",
      "Predict | Day: 24\n",
      "##########  0.46 min round |  10.98 min total |  36557.71 day sales |\n",
      "Predict | Day: 25\n",
      "##########  0.46 min round |  11.44 min total |  36488.92 day sales |\n",
      "Predict | Day: 26\n",
      "##########  0.45 min round |  11.90 min total |  41419.57 day sales |\n",
      "Predict | Day: 27\n",
      "##########  0.45 min round |  12.35 min total |  50296.43 day sales |\n",
      "Predict | Day: 28\n",
      "##########  0.46 min round |  12.81 min total |  50785.47 day sales |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.761383</td>\n",
       "      <td>0.699831</td>\n",
       "      <td>0.692745</td>\n",
       "      <td>0.702481</td>\n",
       "      <td>0.892973</td>\n",
       "      <td>1.005403</td>\n",
       "      <td>1.077389</td>\n",
       "      <td>0.804953</td>\n",
       "      <td>0.822731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721441</td>\n",
       "      <td>1.016850</td>\n",
       "      <td>0.942547</td>\n",
       "      <td>0.745097</td>\n",
       "      <td>0.687822</td>\n",
       "      <td>0.669285</td>\n",
       "      <td>0.716080</td>\n",
       "      <td>0.788798</td>\n",
       "      <td>0.895221</td>\n",
       "      <td>0.879104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.186197</td>\n",
       "      <td>0.166763</td>\n",
       "      <td>0.159230</td>\n",
       "      <td>0.190265</td>\n",
       "      <td>0.220106</td>\n",
       "      <td>0.303975</td>\n",
       "      <td>0.319407</td>\n",
       "      <td>0.245637</td>\n",
       "      <td>0.213171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221486</td>\n",
       "      <td>0.298720</td>\n",
       "      <td>0.305537</td>\n",
       "      <td>0.191720</td>\n",
       "      <td>0.185846</td>\n",
       "      <td>0.171170</td>\n",
       "      <td>0.188095</td>\n",
       "      <td>0.214386</td>\n",
       "      <td>0.303549</td>\n",
       "      <td>0.292198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.386775</td>\n",
       "      <td>0.377378</td>\n",
       "      <td>0.366195</td>\n",
       "      <td>0.382453</td>\n",
       "      <td>0.535730</td>\n",
       "      <td>0.688187</td>\n",
       "      <td>0.570389</td>\n",
       "      <td>0.405425</td>\n",
       "      <td>0.382445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459654</td>\n",
       "      <td>0.592833</td>\n",
       "      <td>0.614374</td>\n",
       "      <td>0.441154</td>\n",
       "      <td>0.376918</td>\n",
       "      <td>0.367238</td>\n",
       "      <td>0.377534</td>\n",
       "      <td>0.478754</td>\n",
       "      <td>0.600869</td>\n",
       "      <td>0.595583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.698843</td>\n",
       "      <td>1.295689</td>\n",
       "      <td>1.310318</td>\n",
       "      <td>1.505870</td>\n",
       "      <td>2.144337</td>\n",
       "      <td>3.048811</td>\n",
       "      <td>3.025877</td>\n",
       "      <td>1.615458</td>\n",
       "      <td>1.346638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.975850</td>\n",
       "      <td>2.627865</td>\n",
       "      <td>3.167604</td>\n",
       "      <td>1.609857</td>\n",
       "      <td>1.411745</td>\n",
       "      <td>1.321483</td>\n",
       "      <td>1.442064</td>\n",
       "      <td>1.937776</td>\n",
       "      <td>2.993916</td>\n",
       "      <td>3.254758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.809201</td>\n",
       "      <td>0.748848</td>\n",
       "      <td>0.813447</td>\n",
       "      <td>0.832193</td>\n",
       "      <td>1.105481</td>\n",
       "      <td>1.574576</td>\n",
       "      <td>1.331610</td>\n",
       "      <td>0.884792</td>\n",
       "      <td>0.962204</td>\n",
       "      <td>...</td>\n",
       "      <td>1.037940</td>\n",
       "      <td>1.366661</td>\n",
       "      <td>1.474921</td>\n",
       "      <td>0.891875</td>\n",
       "      <td>0.790159</td>\n",
       "      <td>0.802558</td>\n",
       "      <td>0.798833</td>\n",
       "      <td>1.097145</td>\n",
       "      <td>1.486299</td>\n",
       "      <td>1.372422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>0.971025</td>\n",
       "      <td>0.933903</td>\n",
       "      <td>0.907718</td>\n",
       "      <td>0.965741</td>\n",
       "      <td>0.861649</td>\n",
       "      <td>1.196083</td>\n",
       "      <td>1.244771</td>\n",
       "      <td>1.015025</td>\n",
       "      <td>0.991325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886830</td>\n",
       "      <td>1.113046</td>\n",
       "      <td>1.070272</td>\n",
       "      <td>0.848680</td>\n",
       "      <td>0.768911</td>\n",
       "      <td>0.863001</td>\n",
       "      <td>0.858521</td>\n",
       "      <td>0.922690</td>\n",
       "      <td>1.094452</td>\n",
       "      <td>0.982139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>0.287608</td>\n",
       "      <td>0.300085</td>\n",
       "      <td>0.264826</td>\n",
       "      <td>0.277345</td>\n",
       "      <td>0.333235</td>\n",
       "      <td>0.378311</td>\n",
       "      <td>0.362494</td>\n",
       "      <td>0.322844</td>\n",
       "      <td>0.301289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335688</td>\n",
       "      <td>0.412788</td>\n",
       "      <td>0.418779</td>\n",
       "      <td>0.316092</td>\n",
       "      <td>0.280861</td>\n",
       "      <td>0.285296</td>\n",
       "      <td>0.315958</td>\n",
       "      <td>0.344669</td>\n",
       "      <td>0.423415</td>\n",
       "      <td>0.430890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>7.520162</td>\n",
       "      <td>8.680940</td>\n",
       "      <td>8.326200</td>\n",
       "      <td>7.445185</td>\n",
       "      <td>8.279196</td>\n",
       "      <td>8.991793</td>\n",
       "      <td>7.443403</td>\n",
       "      <td>8.617073</td>\n",
       "      <td>9.237152</td>\n",
       "      <td>...</td>\n",
       "      <td>8.508981</td>\n",
       "      <td>9.855074</td>\n",
       "      <td>8.998866</td>\n",
       "      <td>8.769915</td>\n",
       "      <td>7.818300</td>\n",
       "      <td>7.645457</td>\n",
       "      <td>8.525203</td>\n",
       "      <td>8.840375</td>\n",
       "      <td>10.189419</td>\n",
       "      <td>7.843835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>0.940546</td>\n",
       "      <td>0.884955</td>\n",
       "      <td>0.747171</td>\n",
       "      <td>0.811557</td>\n",
       "      <td>0.895450</td>\n",
       "      <td>1.093839</td>\n",
       "      <td>1.244978</td>\n",
       "      <td>0.956384</td>\n",
       "      <td>0.969431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895239</td>\n",
       "      <td>1.107968</td>\n",
       "      <td>1.176608</td>\n",
       "      <td>0.967599</td>\n",
       "      <td>0.986313</td>\n",
       "      <td>0.860273</td>\n",
       "      <td>0.854144</td>\n",
       "      <td>0.861564</td>\n",
       "      <td>1.120156</td>\n",
       "      <td>1.129025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>0.652690</td>\n",
       "      <td>0.540803</td>\n",
       "      <td>0.519086</td>\n",
       "      <td>0.475970</td>\n",
       "      <td>0.574691</td>\n",
       "      <td>0.882484</td>\n",
       "      <td>0.836064</td>\n",
       "      <td>0.687134</td>\n",
       "      <td>0.684227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652323</td>\n",
       "      <td>0.813062</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>0.628954</td>\n",
       "      <td>0.584692</td>\n",
       "      <td>0.531422</td>\n",
       "      <td>0.528184</td>\n",
       "      <td>0.643502</td>\n",
       "      <td>0.714531</td>\n",
       "      <td>0.857981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOBBIES_1_011_CA_1_validation</td>\n",
       "      <td>0.100813</td>\n",
       "      <td>0.091885</td>\n",
       "      <td>0.093135</td>\n",
       "      <td>0.090341</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>0.176226</td>\n",
       "      <td>0.155053</td>\n",
       "      <td>0.102096</td>\n",
       "      <td>0.089712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124495</td>\n",
       "      <td>0.174872</td>\n",
       "      <td>0.153820</td>\n",
       "      <td>0.084842</td>\n",
       "      <td>0.079469</td>\n",
       "      <td>0.083373</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>0.115393</td>\n",
       "      <td>0.145076</td>\n",
       "      <td>0.153799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>0.167550</td>\n",
       "      <td>0.167466</td>\n",
       "      <td>0.170872</td>\n",
       "      <td>0.193865</td>\n",
       "      <td>0.232047</td>\n",
       "      <td>0.322741</td>\n",
       "      <td>0.347496</td>\n",
       "      <td>0.213467</td>\n",
       "      <td>0.191851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208906</td>\n",
       "      <td>0.291250</td>\n",
       "      <td>0.334616</td>\n",
       "      <td>0.205388</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.178414</td>\n",
       "      <td>0.176512</td>\n",
       "      <td>0.228829</td>\n",
       "      <td>0.325505</td>\n",
       "      <td>0.311445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HOBBIES_1_013_CA_1_validation</td>\n",
       "      <td>0.297257</td>\n",
       "      <td>0.264511</td>\n",
       "      <td>0.261566</td>\n",
       "      <td>0.279059</td>\n",
       "      <td>0.352816</td>\n",
       "      <td>0.428608</td>\n",
       "      <td>0.528484</td>\n",
       "      <td>0.320989</td>\n",
       "      <td>0.324071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367017</td>\n",
       "      <td>0.513173</td>\n",
       "      <td>0.543729</td>\n",
       "      <td>0.327520</td>\n",
       "      <td>0.311956</td>\n",
       "      <td>0.325282</td>\n",
       "      <td>0.298931</td>\n",
       "      <td>0.383341</td>\n",
       "      <td>0.527665</td>\n",
       "      <td>0.536757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
       "      <td>1.725925</td>\n",
       "      <td>1.546293</td>\n",
       "      <td>1.490278</td>\n",
       "      <td>1.385032</td>\n",
       "      <td>1.660205</td>\n",
       "      <td>2.093942</td>\n",
       "      <td>2.106679</td>\n",
       "      <td>1.791160</td>\n",
       "      <td>1.517616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594316</td>\n",
       "      <td>1.851833</td>\n",
       "      <td>1.712947</td>\n",
       "      <td>1.507791</td>\n",
       "      <td>1.363767</td>\n",
       "      <td>1.456970</td>\n",
       "      <td>1.520317</td>\n",
       "      <td>1.638355</td>\n",
       "      <td>1.895474</td>\n",
       "      <td>1.745547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>3.035540</td>\n",
       "      <td>2.760970</td>\n",
       "      <td>2.873916</td>\n",
       "      <td>2.868697</td>\n",
       "      <td>3.387390</td>\n",
       "      <td>4.734788</td>\n",
       "      <td>4.052949</td>\n",
       "      <td>3.504833</td>\n",
       "      <td>2.944639</td>\n",
       "      <td>...</td>\n",
       "      <td>3.072271</td>\n",
       "      <td>4.641672</td>\n",
       "      <td>4.154986</td>\n",
       "      <td>2.672733</td>\n",
       "      <td>2.605711</td>\n",
       "      <td>2.579292</td>\n",
       "      <td>2.663736</td>\n",
       "      <td>3.438724</td>\n",
       "      <td>5.165877</td>\n",
       "      <td>4.573015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>5.398061</td>\n",
       "      <td>5.360966</td>\n",
       "      <td>5.029590</td>\n",
       "      <td>5.526792</td>\n",
       "      <td>5.574027</td>\n",
       "      <td>7.723866</td>\n",
       "      <td>6.229207</td>\n",
       "      <td>5.681062</td>\n",
       "      <td>5.717836</td>\n",
       "      <td>...</td>\n",
       "      <td>5.300734</td>\n",
       "      <td>7.194888</td>\n",
       "      <td>7.117406</td>\n",
       "      <td>5.706486</td>\n",
       "      <td>4.820916</td>\n",
       "      <td>5.116723</td>\n",
       "      <td>5.094998</td>\n",
       "      <td>5.725630</td>\n",
       "      <td>6.861572</td>\n",
       "      <td>6.954901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HOBBIES_1_017_CA_1_validation</td>\n",
       "      <td>1.002425</td>\n",
       "      <td>0.901999</td>\n",
       "      <td>0.891700</td>\n",
       "      <td>0.894397</td>\n",
       "      <td>1.326626</td>\n",
       "      <td>1.746263</td>\n",
       "      <td>1.848723</td>\n",
       "      <td>1.154462</td>\n",
       "      <td>1.046950</td>\n",
       "      <td>...</td>\n",
       "      <td>1.283830</td>\n",
       "      <td>1.547265</td>\n",
       "      <td>1.707190</td>\n",
       "      <td>0.972338</td>\n",
       "      <td>0.999907</td>\n",
       "      <td>1.025281</td>\n",
       "      <td>0.987732</td>\n",
       "      <td>1.268296</td>\n",
       "      <td>1.912866</td>\n",
       "      <td>1.960343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HOBBIES_1_018_CA_1_validation</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>0.048050</td>\n",
       "      <td>0.051049</td>\n",
       "      <td>0.051426</td>\n",
       "      <td>0.076277</td>\n",
       "      <td>0.105622</td>\n",
       "      <td>0.097334</td>\n",
       "      <td>0.064754</td>\n",
       "      <td>0.057849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>0.102908</td>\n",
       "      <td>0.101147</td>\n",
       "      <td>0.066550</td>\n",
       "      <td>0.061231</td>\n",
       "      <td>0.066247</td>\n",
       "      <td>0.066359</td>\n",
       "      <td>0.091466</td>\n",
       "      <td>0.119707</td>\n",
       "      <td>0.112565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HOBBIES_1_019_CA_1_validation</td>\n",
       "      <td>7.213361</td>\n",
       "      <td>6.198563</td>\n",
       "      <td>6.360021</td>\n",
       "      <td>6.660259</td>\n",
       "      <td>7.636370</td>\n",
       "      <td>7.998594</td>\n",
       "      <td>7.616723</td>\n",
       "      <td>8.473769</td>\n",
       "      <td>8.074746</td>\n",
       "      <td>...</td>\n",
       "      <td>7.064124</td>\n",
       "      <td>8.128608</td>\n",
       "      <td>7.817506</td>\n",
       "      <td>6.825396</td>\n",
       "      <td>6.237270</td>\n",
       "      <td>6.879885</td>\n",
       "      <td>7.447481</td>\n",
       "      <td>8.071275</td>\n",
       "      <td>8.412846</td>\n",
       "      <td>8.524029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HOBBIES_1_020_CA_1_validation</td>\n",
       "      <td>0.260648</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>0.259475</td>\n",
       "      <td>0.280993</td>\n",
       "      <td>0.339514</td>\n",
       "      <td>0.399390</td>\n",
       "      <td>0.349610</td>\n",
       "      <td>0.267497</td>\n",
       "      <td>0.268402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313490</td>\n",
       "      <td>0.369620</td>\n",
       "      <td>0.361711</td>\n",
       "      <td>0.265362</td>\n",
       "      <td>0.271102</td>\n",
       "      <td>0.261885</td>\n",
       "      <td>0.282898</td>\n",
       "      <td>0.325361</td>\n",
       "      <td>0.438547</td>\n",
       "      <td>0.406461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HOBBIES_1_021_CA_1_validation</td>\n",
       "      <td>0.574993</td>\n",
       "      <td>0.639805</td>\n",
       "      <td>0.617256</td>\n",
       "      <td>0.733700</td>\n",
       "      <td>0.840748</td>\n",
       "      <td>0.879088</td>\n",
       "      <td>0.825922</td>\n",
       "      <td>0.684675</td>\n",
       "      <td>0.673782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866179</td>\n",
       "      <td>0.994538</td>\n",
       "      <td>0.979919</td>\n",
       "      <td>0.706948</td>\n",
       "      <td>0.633448</td>\n",
       "      <td>0.687246</td>\n",
       "      <td>0.657579</td>\n",
       "      <td>0.848121</td>\n",
       "      <td>1.034064</td>\n",
       "      <td>0.933128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HOBBIES_1_022_CA_1_validation</td>\n",
       "      <td>0.304480</td>\n",
       "      <td>0.284227</td>\n",
       "      <td>0.292361</td>\n",
       "      <td>0.325897</td>\n",
       "      <td>0.380338</td>\n",
       "      <td>0.487532</td>\n",
       "      <td>0.489588</td>\n",
       "      <td>0.345978</td>\n",
       "      <td>0.307665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334645</td>\n",
       "      <td>0.500844</td>\n",
       "      <td>0.501669</td>\n",
       "      <td>0.303974</td>\n",
       "      <td>0.270698</td>\n",
       "      <td>0.272194</td>\n",
       "      <td>0.284246</td>\n",
       "      <td>0.319839</td>\n",
       "      <td>0.454402</td>\n",
       "      <td>0.419996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HOBBIES_1_023_CA_1_validation</td>\n",
       "      <td>1.285492</td>\n",
       "      <td>1.156509</td>\n",
       "      <td>1.054829</td>\n",
       "      <td>1.124688</td>\n",
       "      <td>1.518837</td>\n",
       "      <td>2.150193</td>\n",
       "      <td>1.952789</td>\n",
       "      <td>1.505270</td>\n",
       "      <td>1.268188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.513276</td>\n",
       "      <td>2.177237</td>\n",
       "      <td>2.175420</td>\n",
       "      <td>1.421105</td>\n",
       "      <td>1.151929</td>\n",
       "      <td>1.154446</td>\n",
       "      <td>1.202803</td>\n",
       "      <td>1.603891</td>\n",
       "      <td>2.240332</td>\n",
       "      <td>2.083898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HOBBIES_1_024_CA_1_validation</td>\n",
       "      <td>0.168727</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.156975</td>\n",
       "      <td>0.153192</td>\n",
       "      <td>0.215602</td>\n",
       "      <td>0.286482</td>\n",
       "      <td>0.235108</td>\n",
       "      <td>0.170709</td>\n",
       "      <td>0.169207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229941</td>\n",
       "      <td>0.344926</td>\n",
       "      <td>0.327764</td>\n",
       "      <td>0.196777</td>\n",
       "      <td>0.186791</td>\n",
       "      <td>0.188629</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>0.261173</td>\n",
       "      <td>0.363726</td>\n",
       "      <td>0.352419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HOBBIES_1_025_CA_1_validation</td>\n",
       "      <td>0.424669</td>\n",
       "      <td>0.396680</td>\n",
       "      <td>0.407872</td>\n",
       "      <td>0.386533</td>\n",
       "      <td>0.614462</td>\n",
       "      <td>0.838404</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>0.417745</td>\n",
       "      <td>0.418563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496927</td>\n",
       "      <td>0.690170</td>\n",
       "      <td>0.846833</td>\n",
       "      <td>0.408909</td>\n",
       "      <td>0.377588</td>\n",
       "      <td>0.377659</td>\n",
       "      <td>0.370682</td>\n",
       "      <td>0.552169</td>\n",
       "      <td>0.795295</td>\n",
       "      <td>0.749187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HOBBIES_1_026_CA_1_validation</td>\n",
       "      <td>0.154680</td>\n",
       "      <td>0.144665</td>\n",
       "      <td>0.149505</td>\n",
       "      <td>0.163672</td>\n",
       "      <td>0.173564</td>\n",
       "      <td>0.217049</td>\n",
       "      <td>0.218279</td>\n",
       "      <td>0.183414</td>\n",
       "      <td>0.168108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165970</td>\n",
       "      <td>0.213643</td>\n",
       "      <td>0.216373</td>\n",
       "      <td>0.168177</td>\n",
       "      <td>0.163752</td>\n",
       "      <td>0.164075</td>\n",
       "      <td>0.167466</td>\n",
       "      <td>0.197256</td>\n",
       "      <td>0.230198</td>\n",
       "      <td>0.220696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HOBBIES_1_027_CA_1_validation</td>\n",
       "      <td>0.332416</td>\n",
       "      <td>0.312295</td>\n",
       "      <td>0.299081</td>\n",
       "      <td>0.327543</td>\n",
       "      <td>0.406705</td>\n",
       "      <td>0.584477</td>\n",
       "      <td>0.579301</td>\n",
       "      <td>0.327824</td>\n",
       "      <td>0.351404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388873</td>\n",
       "      <td>0.490833</td>\n",
       "      <td>0.505338</td>\n",
       "      <td>0.309951</td>\n",
       "      <td>0.279894</td>\n",
       "      <td>0.264266</td>\n",
       "      <td>0.280149</td>\n",
       "      <td>0.333404</td>\n",
       "      <td>0.503356</td>\n",
       "      <td>0.484150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HOBBIES_1_028_CA_1_validation</td>\n",
       "      <td>0.640538</td>\n",
       "      <td>0.550474</td>\n",
       "      <td>0.537496</td>\n",
       "      <td>0.586383</td>\n",
       "      <td>0.762550</td>\n",
       "      <td>0.860918</td>\n",
       "      <td>0.863170</td>\n",
       "      <td>0.784372</td>\n",
       "      <td>0.707070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>0.918682</td>\n",
       "      <td>0.890247</td>\n",
       "      <td>0.778294</td>\n",
       "      <td>0.629065</td>\n",
       "      <td>0.585786</td>\n",
       "      <td>0.674764</td>\n",
       "      <td>0.791105</td>\n",
       "      <td>0.944156</td>\n",
       "      <td>0.874460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HOBBIES_1_029_CA_1_validation</td>\n",
       "      <td>1.534072</td>\n",
       "      <td>1.313486</td>\n",
       "      <td>1.237680</td>\n",
       "      <td>1.367389</td>\n",
       "      <td>1.728543</td>\n",
       "      <td>2.251518</td>\n",
       "      <td>2.054722</td>\n",
       "      <td>1.595185</td>\n",
       "      <td>1.602988</td>\n",
       "      <td>...</td>\n",
       "      <td>1.678564</td>\n",
       "      <td>2.183471</td>\n",
       "      <td>2.096464</td>\n",
       "      <td>1.486763</td>\n",
       "      <td>1.372287</td>\n",
       "      <td>1.274426</td>\n",
       "      <td>1.303284</td>\n",
       "      <td>1.732512</td>\n",
       "      <td>2.062654</td>\n",
       "      <td>2.019172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>HOBBIES_1_030_CA_1_validation</td>\n",
       "      <td>5.042911</td>\n",
       "      <td>3.848267</td>\n",
       "      <td>3.817193</td>\n",
       "      <td>3.586862</td>\n",
       "      <td>5.874573</td>\n",
       "      <td>6.481358</td>\n",
       "      <td>7.488183</td>\n",
       "      <td>5.786495</td>\n",
       "      <td>5.504542</td>\n",
       "      <td>...</td>\n",
       "      <td>6.158967</td>\n",
       "      <td>8.212684</td>\n",
       "      <td>7.900284</td>\n",
       "      <td>5.363672</td>\n",
       "      <td>4.674690</td>\n",
       "      <td>4.559300</td>\n",
       "      <td>4.497843</td>\n",
       "      <td>5.908718</td>\n",
       "      <td>7.855041</td>\n",
       "      <td>7.684653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30460</th>\n",
       "      <td>FOODS_3_798_WI_3_validation</td>\n",
       "      <td>0.222716</td>\n",
       "      <td>0.201203</td>\n",
       "      <td>0.194703</td>\n",
       "      <td>0.208032</td>\n",
       "      <td>0.263237</td>\n",
       "      <td>0.322494</td>\n",
       "      <td>0.293090</td>\n",
       "      <td>0.324141</td>\n",
       "      <td>0.321107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339575</td>\n",
       "      <td>0.437564</td>\n",
       "      <td>0.448087</td>\n",
       "      <td>0.289625</td>\n",
       "      <td>0.266412</td>\n",
       "      <td>0.265517</td>\n",
       "      <td>0.257086</td>\n",
       "      <td>0.313654</td>\n",
       "      <td>0.372139</td>\n",
       "      <td>0.361469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>FOODS_3_799_WI_3_validation</td>\n",
       "      <td>0.138853</td>\n",
       "      <td>0.144163</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.146631</td>\n",
       "      <td>0.183196</td>\n",
       "      <td>0.215838</td>\n",
       "      <td>0.254020</td>\n",
       "      <td>0.266828</td>\n",
       "      <td>0.297285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>0.375367</td>\n",
       "      <td>0.354032</td>\n",
       "      <td>0.281191</td>\n",
       "      <td>0.259960</td>\n",
       "      <td>0.238993</td>\n",
       "      <td>0.203846</td>\n",
       "      <td>0.223653</td>\n",
       "      <td>0.267846</td>\n",
       "      <td>0.246581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30462</th>\n",
       "      <td>FOODS_3_800_WI_3_validation</td>\n",
       "      <td>6.511514</td>\n",
       "      <td>6.313174</td>\n",
       "      <td>5.907671</td>\n",
       "      <td>6.357418</td>\n",
       "      <td>7.710922</td>\n",
       "      <td>10.355521</td>\n",
       "      <td>10.372346</td>\n",
       "      <td>11.452431</td>\n",
       "      <td>11.024379</td>\n",
       "      <td>...</td>\n",
       "      <td>9.755917</td>\n",
       "      <td>12.639675</td>\n",
       "      <td>13.204971</td>\n",
       "      <td>9.059383</td>\n",
       "      <td>7.963255</td>\n",
       "      <td>7.381751</td>\n",
       "      <td>7.380766</td>\n",
       "      <td>8.963111</td>\n",
       "      <td>10.646206</td>\n",
       "      <td>10.138635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>FOODS_3_801_WI_3_validation</td>\n",
       "      <td>2.189624</td>\n",
       "      <td>1.996440</td>\n",
       "      <td>1.960798</td>\n",
       "      <td>2.067014</td>\n",
       "      <td>2.443345</td>\n",
       "      <td>3.179685</td>\n",
       "      <td>2.726630</td>\n",
       "      <td>3.023732</td>\n",
       "      <td>3.344578</td>\n",
       "      <td>...</td>\n",
       "      <td>2.868796</td>\n",
       "      <td>3.710054</td>\n",
       "      <td>3.708735</td>\n",
       "      <td>2.580649</td>\n",
       "      <td>2.322851</td>\n",
       "      <td>2.282520</td>\n",
       "      <td>2.324474</td>\n",
       "      <td>2.721751</td>\n",
       "      <td>2.888090</td>\n",
       "      <td>2.586751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30464</th>\n",
       "      <td>FOODS_3_802_WI_3_validation</td>\n",
       "      <td>0.295317</td>\n",
       "      <td>0.287856</td>\n",
       "      <td>0.319405</td>\n",
       "      <td>0.292028</td>\n",
       "      <td>0.374453</td>\n",
       "      <td>0.488528</td>\n",
       "      <td>0.404026</td>\n",
       "      <td>0.444842</td>\n",
       "      <td>0.512098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481950</td>\n",
       "      <td>0.671080</td>\n",
       "      <td>0.697461</td>\n",
       "      <td>0.427767</td>\n",
       "      <td>0.358714</td>\n",
       "      <td>0.354140</td>\n",
       "      <td>0.325989</td>\n",
       "      <td>0.374801</td>\n",
       "      <td>0.467258</td>\n",
       "      <td>0.445576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30465</th>\n",
       "      <td>FOODS_3_803_WI_3_validation</td>\n",
       "      <td>0.078582</td>\n",
       "      <td>0.727699</td>\n",
       "      <td>0.674194</td>\n",
       "      <td>0.753306</td>\n",
       "      <td>1.080511</td>\n",
       "      <td>1.204430</td>\n",
       "      <td>1.031431</td>\n",
       "      <td>0.926605</td>\n",
       "      <td>1.081711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935645</td>\n",
       "      <td>1.120031</td>\n",
       "      <td>1.099097</td>\n",
       "      <td>0.845807</td>\n",
       "      <td>0.815870</td>\n",
       "      <td>0.751224</td>\n",
       "      <td>0.776558</td>\n",
       "      <td>0.822724</td>\n",
       "      <td>0.968808</td>\n",
       "      <td>0.936198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30466</th>\n",
       "      <td>FOODS_3_804_WI_3_validation</td>\n",
       "      <td>4.767077</td>\n",
       "      <td>4.926737</td>\n",
       "      <td>4.612556</td>\n",
       "      <td>4.651411</td>\n",
       "      <td>5.259550</td>\n",
       "      <td>6.950735</td>\n",
       "      <td>6.281364</td>\n",
       "      <td>7.735649</td>\n",
       "      <td>7.803589</td>\n",
       "      <td>...</td>\n",
       "      <td>7.591762</td>\n",
       "      <td>10.138196</td>\n",
       "      <td>9.586011</td>\n",
       "      <td>6.771342</td>\n",
       "      <td>6.339804</td>\n",
       "      <td>6.602828</td>\n",
       "      <td>6.360319</td>\n",
       "      <td>7.429659</td>\n",
       "      <td>8.116358</td>\n",
       "      <td>7.563882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30467</th>\n",
       "      <td>FOODS_3_805_WI_3_validation</td>\n",
       "      <td>1.582969</td>\n",
       "      <td>1.489955</td>\n",
       "      <td>1.334332</td>\n",
       "      <td>1.432947</td>\n",
       "      <td>1.987196</td>\n",
       "      <td>2.333558</td>\n",
       "      <td>2.027294</td>\n",
       "      <td>2.435288</td>\n",
       "      <td>2.586767</td>\n",
       "      <td>...</td>\n",
       "      <td>2.022234</td>\n",
       "      <td>2.741758</td>\n",
       "      <td>3.005401</td>\n",
       "      <td>2.124398</td>\n",
       "      <td>1.761838</td>\n",
       "      <td>1.782581</td>\n",
       "      <td>1.683735</td>\n",
       "      <td>1.886568</td>\n",
       "      <td>2.364191</td>\n",
       "      <td>2.144896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30468</th>\n",
       "      <td>FOODS_3_806_WI_3_validation</td>\n",
       "      <td>0.568305</td>\n",
       "      <td>0.462002</td>\n",
       "      <td>0.440515</td>\n",
       "      <td>0.428196</td>\n",
       "      <td>0.482510</td>\n",
       "      <td>0.525321</td>\n",
       "      <td>0.652520</td>\n",
       "      <td>0.644913</td>\n",
       "      <td>0.674920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524491</td>\n",
       "      <td>0.814290</td>\n",
       "      <td>0.810781</td>\n",
       "      <td>0.597159</td>\n",
       "      <td>0.507719</td>\n",
       "      <td>0.494005</td>\n",
       "      <td>0.449542</td>\n",
       "      <td>0.497705</td>\n",
       "      <td>0.583536</td>\n",
       "      <td>0.577710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30469</th>\n",
       "      <td>FOODS_3_807_WI_3_validation</td>\n",
       "      <td>2.685382</td>\n",
       "      <td>2.302382</td>\n",
       "      <td>1.756712</td>\n",
       "      <td>1.933028</td>\n",
       "      <td>2.515636</td>\n",
       "      <td>3.234179</td>\n",
       "      <td>2.890555</td>\n",
       "      <td>3.944348</td>\n",
       "      <td>4.326148</td>\n",
       "      <td>...</td>\n",
       "      <td>3.350301</td>\n",
       "      <td>4.679049</td>\n",
       "      <td>4.983021</td>\n",
       "      <td>3.587104</td>\n",
       "      <td>3.170727</td>\n",
       "      <td>2.846358</td>\n",
       "      <td>2.774932</td>\n",
       "      <td>3.018351</td>\n",
       "      <td>3.643750</td>\n",
       "      <td>3.207419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30470</th>\n",
       "      <td>FOODS_3_808_WI_3_validation</td>\n",
       "      <td>0.206476</td>\n",
       "      <td>1.547436</td>\n",
       "      <td>2.746214</td>\n",
       "      <td>4.378733</td>\n",
       "      <td>6.925810</td>\n",
       "      <td>13.431726</td>\n",
       "      <td>13.887268</td>\n",
       "      <td>17.427527</td>\n",
       "      <td>17.293589</td>\n",
       "      <td>...</td>\n",
       "      <td>12.239617</td>\n",
       "      <td>17.907049</td>\n",
       "      <td>18.824364</td>\n",
       "      <td>13.815523</td>\n",
       "      <td>11.381625</td>\n",
       "      <td>9.785874</td>\n",
       "      <td>8.919386</td>\n",
       "      <td>9.877444</td>\n",
       "      <td>12.214796</td>\n",
       "      <td>12.519151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30471</th>\n",
       "      <td>FOODS_3_809_WI_3_validation</td>\n",
       "      <td>2.077839</td>\n",
       "      <td>1.777235</td>\n",
       "      <td>1.696982</td>\n",
       "      <td>1.809905</td>\n",
       "      <td>2.465362</td>\n",
       "      <td>2.611482</td>\n",
       "      <td>2.388802</td>\n",
       "      <td>3.433774</td>\n",
       "      <td>3.253776</td>\n",
       "      <td>...</td>\n",
       "      <td>2.391291</td>\n",
       "      <td>3.241021</td>\n",
       "      <td>2.993778</td>\n",
       "      <td>2.556246</td>\n",
       "      <td>2.075832</td>\n",
       "      <td>1.950172</td>\n",
       "      <td>2.130104</td>\n",
       "      <td>2.481181</td>\n",
       "      <td>2.666826</td>\n",
       "      <td>2.278355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>FOODS_3_810_WI_3_validation</td>\n",
       "      <td>0.712912</td>\n",
       "      <td>0.664378</td>\n",
       "      <td>0.551892</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.733231</td>\n",
       "      <td>0.774948</td>\n",
       "      <td>0.704415</td>\n",
       "      <td>0.706879</td>\n",
       "      <td>0.766795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690083</td>\n",
       "      <td>0.927935</td>\n",
       "      <td>0.888569</td>\n",
       "      <td>0.674735</td>\n",
       "      <td>0.574485</td>\n",
       "      <td>0.562904</td>\n",
       "      <td>0.560398</td>\n",
       "      <td>0.648519</td>\n",
       "      <td>0.745432</td>\n",
       "      <td>0.741875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>FOODS_3_811_WI_3_validation</td>\n",
       "      <td>14.443844</td>\n",
       "      <td>14.025472</td>\n",
       "      <td>12.792300</td>\n",
       "      <td>12.391145</td>\n",
       "      <td>13.469036</td>\n",
       "      <td>17.579513</td>\n",
       "      <td>18.583097</td>\n",
       "      <td>20.137404</td>\n",
       "      <td>19.665488</td>\n",
       "      <td>...</td>\n",
       "      <td>15.626003</td>\n",
       "      <td>22.633553</td>\n",
       "      <td>21.825908</td>\n",
       "      <td>14.403809</td>\n",
       "      <td>12.724426</td>\n",
       "      <td>11.686406</td>\n",
       "      <td>11.589025</td>\n",
       "      <td>12.865177</td>\n",
       "      <td>15.278034</td>\n",
       "      <td>14.438824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30474</th>\n",
       "      <td>FOODS_3_812_WI_3_validation</td>\n",
       "      <td>0.976561</td>\n",
       "      <td>0.945384</td>\n",
       "      <td>0.910824</td>\n",
       "      <td>1.008158</td>\n",
       "      <td>1.281925</td>\n",
       "      <td>1.335416</td>\n",
       "      <td>1.523646</td>\n",
       "      <td>1.734182</td>\n",
       "      <td>1.699767</td>\n",
       "      <td>...</td>\n",
       "      <td>1.609228</td>\n",
       "      <td>2.001386</td>\n",
       "      <td>2.114886</td>\n",
       "      <td>1.323270</td>\n",
       "      <td>1.002999</td>\n",
       "      <td>1.046330</td>\n",
       "      <td>1.084705</td>\n",
       "      <td>1.199420</td>\n",
       "      <td>1.405974</td>\n",
       "      <td>1.465538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>FOODS_3_813_WI_3_validation</td>\n",
       "      <td>0.272310</td>\n",
       "      <td>0.259926</td>\n",
       "      <td>0.257294</td>\n",
       "      <td>0.231930</td>\n",
       "      <td>0.229353</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>0.323403</td>\n",
       "      <td>0.326145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272197</td>\n",
       "      <td>0.411575</td>\n",
       "      <td>0.441697</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>0.330595</td>\n",
       "      <td>0.311910</td>\n",
       "      <td>0.280097</td>\n",
       "      <td>0.275410</td>\n",
       "      <td>0.339140</td>\n",
       "      <td>0.349166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>FOODS_3_814_WI_3_validation</td>\n",
       "      <td>1.978796</td>\n",
       "      <td>1.813624</td>\n",
       "      <td>1.745153</td>\n",
       "      <td>1.869249</td>\n",
       "      <td>2.489955</td>\n",
       "      <td>2.723562</td>\n",
       "      <td>2.519870</td>\n",
       "      <td>2.631889</td>\n",
       "      <td>2.633929</td>\n",
       "      <td>...</td>\n",
       "      <td>2.326858</td>\n",
       "      <td>2.999810</td>\n",
       "      <td>3.123826</td>\n",
       "      <td>2.102449</td>\n",
       "      <td>1.864372</td>\n",
       "      <td>1.791600</td>\n",
       "      <td>1.921084</td>\n",
       "      <td>2.280086</td>\n",
       "      <td>2.821101</td>\n",
       "      <td>2.554536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>FOODS_3_815_WI_3_validation</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.187427</td>\n",
       "      <td>0.175907</td>\n",
       "      <td>0.185331</td>\n",
       "      <td>0.212350</td>\n",
       "      <td>0.308598</td>\n",
       "      <td>0.280282</td>\n",
       "      <td>0.303599</td>\n",
       "      <td>0.328916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252709</td>\n",
       "      <td>0.342865</td>\n",
       "      <td>0.407190</td>\n",
       "      <td>0.234653</td>\n",
       "      <td>0.217945</td>\n",
       "      <td>0.213719</td>\n",
       "      <td>0.213954</td>\n",
       "      <td>0.204197</td>\n",
       "      <td>0.289457</td>\n",
       "      <td>0.325814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30478</th>\n",
       "      <td>FOODS_3_816_WI_3_validation</td>\n",
       "      <td>1.273494</td>\n",
       "      <td>1.571809</td>\n",
       "      <td>1.956114</td>\n",
       "      <td>2.159259</td>\n",
       "      <td>3.358119</td>\n",
       "      <td>4.532821</td>\n",
       "      <td>5.222195</td>\n",
       "      <td>7.970768</td>\n",
       "      <td>8.910274</td>\n",
       "      <td>...</td>\n",
       "      <td>5.251874</td>\n",
       "      <td>7.905098</td>\n",
       "      <td>7.505968</td>\n",
       "      <td>4.755042</td>\n",
       "      <td>4.578028</td>\n",
       "      <td>3.843456</td>\n",
       "      <td>3.926227</td>\n",
       "      <td>5.083600</td>\n",
       "      <td>5.568137</td>\n",
       "      <td>5.407452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479</th>\n",
       "      <td>FOODS_3_817_WI_3_validation</td>\n",
       "      <td>0.255071</td>\n",
       "      <td>0.225474</td>\n",
       "      <td>0.231015</td>\n",
       "      <td>0.200974</td>\n",
       "      <td>0.272025</td>\n",
       "      <td>0.310325</td>\n",
       "      <td>0.283573</td>\n",
       "      <td>0.299527</td>\n",
       "      <td>0.291215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297343</td>\n",
       "      <td>0.436215</td>\n",
       "      <td>0.475835</td>\n",
       "      <td>0.324345</td>\n",
       "      <td>0.273858</td>\n",
       "      <td>0.277854</td>\n",
       "      <td>0.264079</td>\n",
       "      <td>0.291748</td>\n",
       "      <td>0.365469</td>\n",
       "      <td>0.393399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30480</th>\n",
       "      <td>FOODS_3_818_WI_3_validation</td>\n",
       "      <td>1.735397</td>\n",
       "      <td>1.577748</td>\n",
       "      <td>1.535754</td>\n",
       "      <td>1.471614</td>\n",
       "      <td>1.876474</td>\n",
       "      <td>1.942074</td>\n",
       "      <td>1.663524</td>\n",
       "      <td>2.120418</td>\n",
       "      <td>2.219363</td>\n",
       "      <td>...</td>\n",
       "      <td>1.662839</td>\n",
       "      <td>2.213876</td>\n",
       "      <td>2.370372</td>\n",
       "      <td>1.865845</td>\n",
       "      <td>1.517565</td>\n",
       "      <td>1.533176</td>\n",
       "      <td>1.533216</td>\n",
       "      <td>1.796945</td>\n",
       "      <td>1.949751</td>\n",
       "      <td>1.892525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30481</th>\n",
       "      <td>FOODS_3_819_WI_3_validation</td>\n",
       "      <td>1.739451</td>\n",
       "      <td>1.689673</td>\n",
       "      <td>1.707958</td>\n",
       "      <td>1.828195</td>\n",
       "      <td>2.306176</td>\n",
       "      <td>2.550306</td>\n",
       "      <td>2.110372</td>\n",
       "      <td>2.883059</td>\n",
       "      <td>2.883493</td>\n",
       "      <td>...</td>\n",
       "      <td>2.301625</td>\n",
       "      <td>3.051677</td>\n",
       "      <td>3.085664</td>\n",
       "      <td>2.339957</td>\n",
       "      <td>2.032496</td>\n",
       "      <td>1.822832</td>\n",
       "      <td>1.893042</td>\n",
       "      <td>2.021454</td>\n",
       "      <td>2.470101</td>\n",
       "      <td>2.414162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30482</th>\n",
       "      <td>FOODS_3_820_WI_3_validation</td>\n",
       "      <td>1.412018</td>\n",
       "      <td>1.285397</td>\n",
       "      <td>1.339731</td>\n",
       "      <td>1.357643</td>\n",
       "      <td>1.634013</td>\n",
       "      <td>1.796314</td>\n",
       "      <td>1.732007</td>\n",
       "      <td>2.061105</td>\n",
       "      <td>2.061211</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735416</td>\n",
       "      <td>2.241093</td>\n",
       "      <td>2.336207</td>\n",
       "      <td>1.676307</td>\n",
       "      <td>1.505225</td>\n",
       "      <td>1.391123</td>\n",
       "      <td>1.461886</td>\n",
       "      <td>1.617678</td>\n",
       "      <td>1.775660</td>\n",
       "      <td>1.626835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30483</th>\n",
       "      <td>FOODS_3_821_WI_3_validation</td>\n",
       "      <td>0.786267</td>\n",
       "      <td>0.708388</td>\n",
       "      <td>0.685408</td>\n",
       "      <td>0.751362</td>\n",
       "      <td>0.920953</td>\n",
       "      <td>1.160651</td>\n",
       "      <td>1.133500</td>\n",
       "      <td>0.979189</td>\n",
       "      <td>1.071979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987544</td>\n",
       "      <td>1.411089</td>\n",
       "      <td>1.550523</td>\n",
       "      <td>0.958467</td>\n",
       "      <td>0.789993</td>\n",
       "      <td>0.808506</td>\n",
       "      <td>0.804694</td>\n",
       "      <td>0.909438</td>\n",
       "      <td>1.156622</td>\n",
       "      <td>1.226016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>FOODS_3_822_WI_3_validation</td>\n",
       "      <td>1.996696</td>\n",
       "      <td>1.881083</td>\n",
       "      <td>1.616973</td>\n",
       "      <td>1.800300</td>\n",
       "      <td>2.181031</td>\n",
       "      <td>2.579376</td>\n",
       "      <td>2.789281</td>\n",
       "      <td>3.064641</td>\n",
       "      <td>3.073762</td>\n",
       "      <td>...</td>\n",
       "      <td>2.602368</td>\n",
       "      <td>3.599866</td>\n",
       "      <td>4.103986</td>\n",
       "      <td>2.808497</td>\n",
       "      <td>2.358402</td>\n",
       "      <td>2.240874</td>\n",
       "      <td>2.113799</td>\n",
       "      <td>2.454133</td>\n",
       "      <td>2.881576</td>\n",
       "      <td>3.126139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_validation</td>\n",
       "      <td>0.430603</td>\n",
       "      <td>0.371188</td>\n",
       "      <td>0.410488</td>\n",
       "      <td>0.439088</td>\n",
       "      <td>0.501427</td>\n",
       "      <td>0.566079</td>\n",
       "      <td>0.565262</td>\n",
       "      <td>0.632725</td>\n",
       "      <td>0.632416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589997</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.750119</td>\n",
       "      <td>0.628249</td>\n",
       "      <td>0.501595</td>\n",
       "      <td>0.457645</td>\n",
       "      <td>0.420612</td>\n",
       "      <td>0.522497</td>\n",
       "      <td>0.615650</td>\n",
       "      <td>0.694607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_validation</td>\n",
       "      <td>0.296921</td>\n",
       "      <td>0.243446</td>\n",
       "      <td>0.222207</td>\n",
       "      <td>0.217871</td>\n",
       "      <td>0.258667</td>\n",
       "      <td>0.343395</td>\n",
       "      <td>0.342943</td>\n",
       "      <td>0.486446</td>\n",
       "      <td>0.497309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290207</td>\n",
       "      <td>0.459080</td>\n",
       "      <td>0.532966</td>\n",
       "      <td>0.336838</td>\n",
       "      <td>0.262161</td>\n",
       "      <td>0.239143</td>\n",
       "      <td>0.233151</td>\n",
       "      <td>0.251378</td>\n",
       "      <td>0.361941</td>\n",
       "      <td>0.370644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_validation</td>\n",
       "      <td>0.623696</td>\n",
       "      <td>0.525730</td>\n",
       "      <td>0.512229</td>\n",
       "      <td>0.540402</td>\n",
       "      <td>0.699621</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>0.792196</td>\n",
       "      <td>1.107019</td>\n",
       "      <td>1.031465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954291</td>\n",
       "      <td>1.521635</td>\n",
       "      <td>1.686902</td>\n",
       "      <td>1.075765</td>\n",
       "      <td>0.765166</td>\n",
       "      <td>0.692651</td>\n",
       "      <td>0.620294</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.850391</td>\n",
       "      <td>0.849778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_validation</td>\n",
       "      <td>0.899699</td>\n",
       "      <td>0.833744</td>\n",
       "      <td>0.787884</td>\n",
       "      <td>0.773059</td>\n",
       "      <td>0.910110</td>\n",
       "      <td>1.105545</td>\n",
       "      <td>1.081807</td>\n",
       "      <td>1.090013</td>\n",
       "      <td>1.084925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897696</td>\n",
       "      <td>1.240921</td>\n",
       "      <td>1.409086</td>\n",
       "      <td>0.889050</td>\n",
       "      <td>0.838180</td>\n",
       "      <td>0.786347</td>\n",
       "      <td>0.825422</td>\n",
       "      <td>0.970517</td>\n",
       "      <td>1.072024</td>\n",
       "      <td>1.124424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_validation</td>\n",
       "      <td>0.178113</td>\n",
       "      <td>0.976064</td>\n",
       "      <td>1.000744</td>\n",
       "      <td>1.433127</td>\n",
       "      <td>2.021718</td>\n",
       "      <td>2.574217</td>\n",
       "      <td>1.877494</td>\n",
       "      <td>1.910781</td>\n",
       "      <td>1.850098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.743256</td>\n",
       "      <td>2.249144</td>\n",
       "      <td>2.103810</td>\n",
       "      <td>1.496749</td>\n",
       "      <td>1.428751</td>\n",
       "      <td>1.342826</td>\n",
       "      <td>1.318914</td>\n",
       "      <td>1.573374</td>\n",
       "      <td>1.981285</td>\n",
       "      <td>2.022792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id         F1         F2         F3  \\\n",
       "0      HOBBIES_1_001_CA_1_validation   0.761383   0.699831   0.692745   \n",
       "1      HOBBIES_1_002_CA_1_validation   0.186197   0.166763   0.159230   \n",
       "2      HOBBIES_1_003_CA_1_validation   0.386775   0.377378   0.366195   \n",
       "3      HOBBIES_1_004_CA_1_validation   1.698843   1.295689   1.310318   \n",
       "4      HOBBIES_1_005_CA_1_validation   0.809201   0.748848   0.813447   \n",
       "5      HOBBIES_1_006_CA_1_validation   0.971025   0.933903   0.907718   \n",
       "6      HOBBIES_1_007_CA_1_validation   0.287608   0.300085   0.264826   \n",
       "7      HOBBIES_1_008_CA_1_validation   7.520162   8.680940   8.326200   \n",
       "8      HOBBIES_1_009_CA_1_validation   0.940546   0.884955   0.747171   \n",
       "9      HOBBIES_1_010_CA_1_validation   0.652690   0.540803   0.519086   \n",
       "10     HOBBIES_1_011_CA_1_validation   0.100813   0.091885   0.093135   \n",
       "11     HOBBIES_1_012_CA_1_validation   0.167550   0.167466   0.170872   \n",
       "12     HOBBIES_1_013_CA_1_validation   0.297257   0.264511   0.261566   \n",
       "13     HOBBIES_1_014_CA_1_validation   1.725925   1.546293   1.490278   \n",
       "14     HOBBIES_1_015_CA_1_validation   3.035540   2.760970   2.873916   \n",
       "15     HOBBIES_1_016_CA_1_validation   5.398061   5.360966   5.029590   \n",
       "16     HOBBIES_1_017_CA_1_validation   1.002425   0.901999   0.891700   \n",
       "17     HOBBIES_1_018_CA_1_validation   0.006106   0.048050   0.051049   \n",
       "18     HOBBIES_1_019_CA_1_validation   7.213361   6.198563   6.360021   \n",
       "19     HOBBIES_1_020_CA_1_validation   0.260648   0.261201   0.259475   \n",
       "20     HOBBIES_1_021_CA_1_validation   0.574993   0.639805   0.617256   \n",
       "21     HOBBIES_1_022_CA_1_validation   0.304480   0.284227   0.292361   \n",
       "22     HOBBIES_1_023_CA_1_validation   1.285492   1.156509   1.054829   \n",
       "23     HOBBIES_1_024_CA_1_validation   0.168727   0.153100   0.156975   \n",
       "24     HOBBIES_1_025_CA_1_validation   0.424669   0.396680   0.407872   \n",
       "25     HOBBIES_1_026_CA_1_validation   0.154680   0.144665   0.149505   \n",
       "26     HOBBIES_1_027_CA_1_validation   0.332416   0.312295   0.299081   \n",
       "27     HOBBIES_1_028_CA_1_validation   0.640538   0.550474   0.537496   \n",
       "28     HOBBIES_1_029_CA_1_validation   1.534072   1.313486   1.237680   \n",
       "29     HOBBIES_1_030_CA_1_validation   5.042911   3.848267   3.817193   \n",
       "...                              ...        ...        ...        ...   \n",
       "30460    FOODS_3_798_WI_3_validation   0.222716   0.201203   0.194703   \n",
       "30461    FOODS_3_799_WI_3_validation   0.138853   0.144163   0.135120   \n",
       "30462    FOODS_3_800_WI_3_validation   6.511514   6.313174   5.907671   \n",
       "30463    FOODS_3_801_WI_3_validation   2.189624   1.996440   1.960798   \n",
       "30464    FOODS_3_802_WI_3_validation   0.295317   0.287856   0.319405   \n",
       "30465    FOODS_3_803_WI_3_validation   0.078582   0.727699   0.674194   \n",
       "30466    FOODS_3_804_WI_3_validation   4.767077   4.926737   4.612556   \n",
       "30467    FOODS_3_805_WI_3_validation   1.582969   1.489955   1.334332   \n",
       "30468    FOODS_3_806_WI_3_validation   0.568305   0.462002   0.440515   \n",
       "30469    FOODS_3_807_WI_3_validation   2.685382   2.302382   1.756712   \n",
       "30470    FOODS_3_808_WI_3_validation   0.206476   1.547436   2.746214   \n",
       "30471    FOODS_3_809_WI_3_validation   2.077839   1.777235   1.696982   \n",
       "30472    FOODS_3_810_WI_3_validation   0.712912   0.664378   0.551892   \n",
       "30473    FOODS_3_811_WI_3_validation  14.443844  14.025472  12.792300   \n",
       "30474    FOODS_3_812_WI_3_validation   0.976561   0.945384   0.910824   \n",
       "30475    FOODS_3_813_WI_3_validation   0.272310   0.259926   0.257294   \n",
       "30476    FOODS_3_814_WI_3_validation   1.978796   1.813624   1.745153   \n",
       "30477    FOODS_3_815_WI_3_validation   0.179060   0.187427   0.175907   \n",
       "30478    FOODS_3_816_WI_3_validation   1.273494   1.571809   1.956114   \n",
       "30479    FOODS_3_817_WI_3_validation   0.255071   0.225474   0.231015   \n",
       "30480    FOODS_3_818_WI_3_validation   1.735397   1.577748   1.535754   \n",
       "30481    FOODS_3_819_WI_3_validation   1.739451   1.689673   1.707958   \n",
       "30482    FOODS_3_820_WI_3_validation   1.412018   1.285397   1.339731   \n",
       "30483    FOODS_3_821_WI_3_validation   0.786267   0.708388   0.685408   \n",
       "30484    FOODS_3_822_WI_3_validation   1.996696   1.881083   1.616973   \n",
       "30485    FOODS_3_823_WI_3_validation   0.430603   0.371188   0.410488   \n",
       "30486    FOODS_3_824_WI_3_validation   0.296921   0.243446   0.222207   \n",
       "30487    FOODS_3_825_WI_3_validation   0.623696   0.525730   0.512229   \n",
       "30488    FOODS_3_826_WI_3_validation   0.899699   0.833744   0.787884   \n",
       "30489    FOODS_3_827_WI_3_validation   0.178113   0.976064   1.000744   \n",
       "\n",
       "              F4         F5         F6         F7         F8         F9  ...  \\\n",
       "0       0.702481   0.892973   1.005403   1.077389   0.804953   0.822731  ...   \n",
       "1       0.190265   0.220106   0.303975   0.319407   0.245637   0.213171  ...   \n",
       "2       0.382453   0.535730   0.688187   0.570389   0.405425   0.382445  ...   \n",
       "3       1.505870   2.144337   3.048811   3.025877   1.615458   1.346638  ...   \n",
       "4       0.832193   1.105481   1.574576   1.331610   0.884792   0.962204  ...   \n",
       "5       0.965741   0.861649   1.196083   1.244771   1.015025   0.991325  ...   \n",
       "6       0.277345   0.333235   0.378311   0.362494   0.322844   0.301289  ...   \n",
       "7       7.445185   8.279196   8.991793   7.443403   8.617073   9.237152  ...   \n",
       "8       0.811557   0.895450   1.093839   1.244978   0.956384   0.969431  ...   \n",
       "9       0.475970   0.574691   0.882484   0.836064   0.687134   0.684227  ...   \n",
       "10      0.090341   0.125340   0.176226   0.155053   0.102096   0.089712  ...   \n",
       "11      0.193865   0.232047   0.322741   0.347496   0.213467   0.191851  ...   \n",
       "12      0.279059   0.352816   0.428608   0.528484   0.320989   0.324071  ...   \n",
       "13      1.385032   1.660205   2.093942   2.106679   1.791160   1.517616  ...   \n",
       "14      2.868697   3.387390   4.734788   4.052949   3.504833   2.944639  ...   \n",
       "15      5.526792   5.574027   7.723866   6.229207   5.681062   5.717836  ...   \n",
       "16      0.894397   1.326626   1.746263   1.848723   1.154462   1.046950  ...   \n",
       "17      0.051426   0.076277   0.105622   0.097334   0.064754   0.057849  ...   \n",
       "18      6.660259   7.636370   7.998594   7.616723   8.473769   8.074746  ...   \n",
       "19      0.280993   0.339514   0.399390   0.349610   0.267497   0.268402  ...   \n",
       "20      0.733700   0.840748   0.879088   0.825922   0.684675   0.673782  ...   \n",
       "21      0.325897   0.380338   0.487532   0.489588   0.345978   0.307665  ...   \n",
       "22      1.124688   1.518837   2.150193   1.952789   1.505270   1.268188  ...   \n",
       "23      0.153192   0.215602   0.286482   0.235108   0.170709   0.169207  ...   \n",
       "24      0.386533   0.614462   0.838404   0.891561   0.417745   0.418563  ...   \n",
       "25      0.163672   0.173564   0.217049   0.218279   0.183414   0.168108  ...   \n",
       "26      0.327543   0.406705   0.584477   0.579301   0.327824   0.351404  ...   \n",
       "27      0.586383   0.762550   0.860918   0.863170   0.784372   0.707070  ...   \n",
       "28      1.367389   1.728543   2.251518   2.054722   1.595185   1.602988  ...   \n",
       "29      3.586862   5.874573   6.481358   7.488183   5.786495   5.504542  ...   \n",
       "...          ...        ...        ...        ...        ...        ...  ...   \n",
       "30460   0.208032   0.263237   0.322494   0.293090   0.324141   0.321107  ...   \n",
       "30461   0.146631   0.183196   0.215838   0.254020   0.266828   0.297285  ...   \n",
       "30462   6.357418   7.710922  10.355521  10.372346  11.452431  11.024379  ...   \n",
       "30463   2.067014   2.443345   3.179685   2.726630   3.023732   3.344578  ...   \n",
       "30464   0.292028   0.374453   0.488528   0.404026   0.444842   0.512098  ...   \n",
       "30465   0.753306   1.080511   1.204430   1.031431   0.926605   1.081711  ...   \n",
       "30466   4.651411   5.259550   6.950735   6.281364   7.735649   7.803589  ...   \n",
       "30467   1.432947   1.987196   2.333558   2.027294   2.435288   2.586767  ...   \n",
       "30468   0.428196   0.482510   0.525321   0.652520   0.644913   0.674920  ...   \n",
       "30469   1.933028   2.515636   3.234179   2.890555   3.944348   4.326148  ...   \n",
       "30470   4.378733   6.925810  13.431726  13.887268  17.427527  17.293589  ...   \n",
       "30471   1.809905   2.465362   2.611482   2.388802   3.433774   3.253776  ...   \n",
       "30472   0.586420   0.733231   0.774948   0.704415   0.706879   0.766795  ...   \n",
       "30473  12.391145  13.469036  17.579513  18.583097  20.137404  19.665488  ...   \n",
       "30474   1.008158   1.281925   1.335416   1.523646   1.734182   1.699767  ...   \n",
       "30475   0.231930   0.229353   0.264600   0.316218   0.323403   0.326145  ...   \n",
       "30476   1.869249   2.489955   2.723562   2.519870   2.631889   2.633929  ...   \n",
       "30477   0.185331   0.212350   0.308598   0.280282   0.303599   0.328916  ...   \n",
       "30478   2.159259   3.358119   4.532821   5.222195   7.970768   8.910274  ...   \n",
       "30479   0.200974   0.272025   0.310325   0.283573   0.299527   0.291215  ...   \n",
       "30480   1.471614   1.876474   1.942074   1.663524   2.120418   2.219363  ...   \n",
       "30481   1.828195   2.306176   2.550306   2.110372   2.883059   2.883493  ...   \n",
       "30482   1.357643   1.634013   1.796314   1.732007   2.061105   2.061211  ...   \n",
       "30483   0.751362   0.920953   1.160651   1.133500   0.979189   1.071979  ...   \n",
       "30484   1.800300   2.181031   2.579376   2.789281   3.064641   3.073762  ...   \n",
       "30485   0.439088   0.501427   0.566079   0.565262   0.632725   0.632416  ...   \n",
       "30486   0.217871   0.258667   0.343395   0.342943   0.486446   0.497309  ...   \n",
       "30487   0.540402   0.699621   0.791395   0.792196   1.107019   1.031465  ...   \n",
       "30488   0.773059   0.910110   1.105545   1.081807   1.090013   1.084925  ...   \n",
       "30489   1.433127   2.021718   2.574217   1.877494   1.910781   1.850098  ...   \n",
       "\n",
       "             F19        F20        F21        F22        F23        F24  \\\n",
       "0       0.721441   1.016850   0.942547   0.745097   0.687822   0.669285   \n",
       "1       0.221486   0.298720   0.305537   0.191720   0.185846   0.171170   \n",
       "2       0.459654   0.592833   0.614374   0.441154   0.376918   0.367238   \n",
       "3       1.975850   2.627865   3.167604   1.609857   1.411745   1.321483   \n",
       "4       1.037940   1.366661   1.474921   0.891875   0.790159   0.802558   \n",
       "5       0.886830   1.113046   1.070272   0.848680   0.768911   0.863001   \n",
       "6       0.335688   0.412788   0.418779   0.316092   0.280861   0.285296   \n",
       "7       8.508981   9.855074   8.998866   8.769915   7.818300   7.645457   \n",
       "8       0.895239   1.107968   1.176608   0.967599   0.986313   0.860273   \n",
       "9       0.652323   0.813062   0.929900   0.628954   0.584692   0.531422   \n",
       "10      0.124495   0.174872   0.153820   0.084842   0.079469   0.083373   \n",
       "11      0.208906   0.291250   0.334616   0.205388   0.184211   0.178414   \n",
       "12      0.367017   0.513173   0.543729   0.327520   0.311956   0.325282   \n",
       "13      1.594316   1.851833   1.712947   1.507791   1.363767   1.456970   \n",
       "14      3.072271   4.641672   4.154986   2.672733   2.605711   2.579292   \n",
       "15      5.300734   7.194888   7.117406   5.706486   4.820916   5.116723   \n",
       "16      1.283830   1.547265   1.707190   0.972338   0.999907   1.025281   \n",
       "17      0.078629   0.102908   0.101147   0.066550   0.061231   0.066247   \n",
       "18      7.064124   8.128608   7.817506   6.825396   6.237270   6.879885   \n",
       "19      0.313490   0.369620   0.361711   0.265362   0.271102   0.261885   \n",
       "20      0.866179   0.994538   0.979919   0.706948   0.633448   0.687246   \n",
       "21      0.334645   0.500844   0.501669   0.303974   0.270698   0.272194   \n",
       "22      1.513276   2.177237   2.175420   1.421105   1.151929   1.154446   \n",
       "23      0.229941   0.344926   0.327764   0.196777   0.186791   0.188629   \n",
       "24      0.496927   0.690170   0.846833   0.408909   0.377588   0.377659   \n",
       "25      0.165970   0.213643   0.216373   0.168177   0.163752   0.164075   \n",
       "26      0.388873   0.490833   0.505338   0.309951   0.279894   0.264266   \n",
       "27      0.914220   0.918682   0.890247   0.778294   0.629065   0.585786   \n",
       "28      1.678564   2.183471   2.096464   1.486763   1.372287   1.274426   \n",
       "29      6.158967   8.212684   7.900284   5.363672   4.674690   4.559300   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "30460   0.339575   0.437564   0.448087   0.289625   0.266412   0.265517   \n",
       "30461   0.298973   0.375367   0.354032   0.281191   0.259960   0.238993   \n",
       "30462   9.755917  12.639675  13.204971   9.059383   7.963255   7.381751   \n",
       "30463   2.868796   3.710054   3.708735   2.580649   2.322851   2.282520   \n",
       "30464   0.481950   0.671080   0.697461   0.427767   0.358714   0.354140   \n",
       "30465   0.935645   1.120031   1.099097   0.845807   0.815870   0.751224   \n",
       "30466   7.591762  10.138196   9.586011   6.771342   6.339804   6.602828   \n",
       "30467   2.022234   2.741758   3.005401   2.124398   1.761838   1.782581   \n",
       "30468   0.524491   0.814290   0.810781   0.597159   0.507719   0.494005   \n",
       "30469   3.350301   4.679049   4.983021   3.587104   3.170727   2.846358   \n",
       "30470  12.239617  17.907049  18.824364  13.815523  11.381625   9.785874   \n",
       "30471   2.391291   3.241021   2.993778   2.556246   2.075832   1.950172   \n",
       "30472   0.690083   0.927935   0.888569   0.674735   0.574485   0.562904   \n",
       "30473  15.626003  22.633553  21.825908  14.403809  12.724426  11.686406   \n",
       "30474   1.609228   2.001386   2.114886   1.323270   1.002999   1.046330   \n",
       "30475   0.272197   0.411575   0.441697   0.363361   0.330595   0.311910   \n",
       "30476   2.326858   2.999810   3.123826   2.102449   1.864372   1.791600   \n",
       "30477   0.252709   0.342865   0.407190   0.234653   0.217945   0.213719   \n",
       "30478   5.251874   7.905098   7.505968   4.755042   4.578028   3.843456   \n",
       "30479   0.297343   0.436215   0.475835   0.324345   0.273858   0.277854   \n",
       "30480   1.662839   2.213876   2.370372   1.865845   1.517565   1.533176   \n",
       "30481   2.301625   3.051677   3.085664   2.339957   2.032496   1.822832   \n",
       "30482   1.735416   2.241093   2.336207   1.676307   1.505225   1.391123   \n",
       "30483   0.987544   1.411089   1.550523   0.958467   0.789993   0.808506   \n",
       "30484   2.602368   3.599866   4.103986   2.808497   2.358402   2.240874   \n",
       "30485   0.589997   0.711268   0.750119   0.628249   0.501595   0.457645   \n",
       "30486   0.290207   0.459080   0.532966   0.336838   0.262161   0.239143   \n",
       "30487   0.954291   1.521635   1.686902   1.075765   0.765166   0.692651   \n",
       "30488   0.897696   1.240921   1.409086   0.889050   0.838180   0.786347   \n",
       "30489   1.743256   2.249144   2.103810   1.496749   1.428751   1.342826   \n",
       "\n",
       "             F25        F26        F27        F28  \n",
       "0       0.716080   0.788798   0.895221   0.879104  \n",
       "1       0.188095   0.214386   0.303549   0.292198  \n",
       "2       0.377534   0.478754   0.600869   0.595583  \n",
       "3       1.442064   1.937776   2.993916   3.254758  \n",
       "4       0.798833   1.097145   1.486299   1.372422  \n",
       "5       0.858521   0.922690   1.094452   0.982139  \n",
       "6       0.315958   0.344669   0.423415   0.430890  \n",
       "7       8.525203   8.840375  10.189419   7.843835  \n",
       "8       0.854144   0.861564   1.120156   1.129025  \n",
       "9       0.528184   0.643502   0.714531   0.857981  \n",
       "10      0.087846   0.115393   0.145076   0.153799  \n",
       "11      0.176512   0.228829   0.325505   0.311445  \n",
       "12      0.298931   0.383341   0.527665   0.536757  \n",
       "13      1.520317   1.638355   1.895474   1.745547  \n",
       "14      2.663736   3.438724   5.165877   4.573015  \n",
       "15      5.094998   5.725630   6.861572   6.954901  \n",
       "16      0.987732   1.268296   1.912866   1.960343  \n",
       "17      0.066359   0.091466   0.119707   0.112565  \n",
       "18      7.447481   8.071275   8.412846   8.524029  \n",
       "19      0.282898   0.325361   0.438547   0.406461  \n",
       "20      0.657579   0.848121   1.034064   0.933128  \n",
       "21      0.284246   0.319839   0.454402   0.419996  \n",
       "22      1.202803   1.603891   2.240332   2.083898  \n",
       "23      0.189815   0.261173   0.363726   0.352419  \n",
       "24      0.370682   0.552169   0.795295   0.749187  \n",
       "25      0.167466   0.197256   0.230198   0.220696  \n",
       "26      0.280149   0.333404   0.503356   0.484150  \n",
       "27      0.674764   0.791105   0.944156   0.874460  \n",
       "28      1.303284   1.732512   2.062654   2.019172  \n",
       "29      4.497843   5.908718   7.855041   7.684653  \n",
       "...          ...        ...        ...        ...  \n",
       "30460   0.257086   0.313654   0.372139   0.361469  \n",
       "30461   0.203846   0.223653   0.267846   0.246581  \n",
       "30462   7.380766   8.963111  10.646206  10.138635  \n",
       "30463   2.324474   2.721751   2.888090   2.586751  \n",
       "30464   0.325989   0.374801   0.467258   0.445576  \n",
       "30465   0.776558   0.822724   0.968808   0.936198  \n",
       "30466   6.360319   7.429659   8.116358   7.563882  \n",
       "30467   1.683735   1.886568   2.364191   2.144896  \n",
       "30468   0.449542   0.497705   0.583536   0.577710  \n",
       "30469   2.774932   3.018351   3.643750   3.207419  \n",
       "30470   8.919386   9.877444  12.214796  12.519151  \n",
       "30471   2.130104   2.481181   2.666826   2.278355  \n",
       "30472   0.560398   0.648519   0.745432   0.741875  \n",
       "30473  11.589025  12.865177  15.278034  14.438824  \n",
       "30474   1.084705   1.199420   1.405974   1.465538  \n",
       "30475   0.280097   0.275410   0.339140   0.349166  \n",
       "30476   1.921084   2.280086   2.821101   2.554536  \n",
       "30477   0.213954   0.204197   0.289457   0.325814  \n",
       "30478   3.926227   5.083600   5.568137   5.407452  \n",
       "30479   0.264079   0.291748   0.365469   0.393399  \n",
       "30480   1.533216   1.796945   1.949751   1.892525  \n",
       "30481   1.893042   2.021454   2.470101   2.414162  \n",
       "30482   1.461886   1.617678   1.775660   1.626835  \n",
       "30483   0.804694   0.909438   1.156622   1.226016  \n",
       "30484   2.113799   2.454133   2.881576   3.126139  \n",
       "30485   0.420612   0.522497   0.615650   0.694607  \n",
       "30486   0.233151   0.251378   0.361941   0.370644  \n",
       "30487   0.620294   0.681327   0.850391   0.849778  \n",
       "30488   0.825422   0.970517   1.072024   1.124424  \n",
       "30489   1.318914   1.573374   1.981285   2.022792  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv.gz')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission_v'+str(VER)+str(SEED)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "# Of course here is no magic at all.\n",
    "# No \"Novel\" features and no brilliant ideas.\n",
    "# We just carefully joined all\n",
    "# our previous fe work and created a model.\n",
    "\n",
    "# Also!\n",
    "# In my opinion this strategy is a \"dead end\".\n",
    "# Overfits a lot LB and with 1 final submission \n",
    "# you have no option to risk.\n",
    "\n",
    "\n",
    "# Improvement should come from:\n",
    "# Loss function\n",
    "# Data representation\n",
    "# Stable CV\n",
    "# Good features reduction strategy\n",
    "# Predictions stabilization with NN\n",
    "# Trend prediction\n",
    "# Real zero sales detection/classification\n",
    "\n",
    "\n",
    "# Good kernels references \n",
    "## (the order is random and the list is not complete):\n",
    "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
    "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
    "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
    "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
    "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
    "# https://www.kaggle.com/yassinealouini/seq2seq\n",
    "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
    "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
    "\n",
    "\n",
    "# Features were created in these kernels:\n",
    "## \n",
    "# Mean encodings and PCA options\n",
    "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
    "##\n",
    "# Lags and rolling lags\n",
    "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "##\n",
    "# Base Grid and base features (calendar/price/etc)\n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "\n",
    "\n",
    "# Personal request\n",
    "# Please don't upvote any ensemble and copypaste kernels\n",
    "## The worst case is ensemble without any analyse.\n",
    "## The best choice - just ignore it.\n",
    "## I would like to see more kernels with interesting and original approaches.\n",
    "## Don't feed copypasters with upvotes.\n",
    "\n",
    "## It doesn't mean that you should not fork and improve others kernels\n",
    "## but I would like to see params and code tuning based on some CV and analyse\n",
    "## and not only on LB probing.\n",
    "## Small changes could be shared in comments and authors can improve their kernel.\n",
    "\n",
    "## Feel free to criticize this kernel as my knowlege is very limited\n",
    "## and I can be wrong in code and descriptions. \n",
    "## Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
