{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=915):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "## Multiprocess Runs\n",
    "def df_parallelize_run(func, t_split):\n",
    "    num_cores = np.min([N_CORES,len(t_split)])\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]\n",
    "\n",
    "\n",
    "def make_lag_roll(LAG_DAY):\n",
    "    shift_day = LAG_DAY[0]\n",
    "    roll_wind = LAG_DAY[1]\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "# 'max_bin': 100\n",
    "\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514\n",
    "# not our case but good to know cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_CORES=72\n"
     ]
    }
   ],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 1                          # Our model version\n",
    "SEED = 915                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "print(f'N_CORES={N_CORES}')\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = './'\n",
    "BASE     = './grid_part_1.pkl.gz'\n",
    "PRICE    = './grid_part_2.pkl.gz'\n",
    "CALENDAR = './grid_part_3.pkl.gz'\n",
    "LAGS     = './lags_df_28.pkl.gz'\n",
    "MEAN_ENC = './mean_encoding_df.pkl.gz'\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "AUX_MODELS = './m5-aux-models/'\n",
    "\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv.gz')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,30,60]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Aux Models\n",
    "# If you don't want to wait hours and hours\n",
    "# to have result you can train each store \n",
    "# in separate kernel and then just join result.\n",
    "\n",
    "# If we want to use pretrained models we can \n",
    "## skip training \n",
    "## (in our case do dummy training\n",
    "##  to show that we are good with memory\n",
    "##  and you can safely use this (all kernel) code)\n",
    "if USE_AUX:\n",
    "    lgb_params['n_estimators'] = 2\n",
    "    \n",
    "# Here is some 'logs' that can compare\n",
    "#Train CA_1\n",
    "#[100]\tvalid_0's rmse: 2.02289\n",
    "#[200]\tvalid_0's rmse: 2.0017\n",
    "#[300]\tvalid_0's rmse: 1.99239\n",
    "#[400]\tvalid_0's rmse: 1.98471\n",
    "#[500]\tvalid_0's rmse: 1.97923\n",
    "#[600]\tvalid_0's rmse: 1.97284\n",
    "#[700]\tvalid_0's rmse: 1.96763\n",
    "#[800]\tvalid_0's rmse: 1.9624\n",
    "#[900]\tvalid_0's rmse: 1.95673\n",
    "#[1000]\tvalid_0's rmse: 1.95201\n",
    "#[1100]\tvalid_0's rmse: 1.9476\n",
    "#[1200]\tvalid_0's rmse: 1.9434\n",
    "#[1300]\tvalid_0's rmse: 1.9392\n",
    "#[1400]\tvalid_0's rmse: 1.93446\n",
    "\n",
    "#Train CA_2\n",
    "#[100]\tvalid_0's rmse: 1.88949\n",
    "#[200]\tvalid_0's rmse: 1.84767\n",
    "#[300]\tvalid_0's rmse: 1.83653\n",
    "#[400]\tvalid_0's rmse: 1.82909\n",
    "#[500]\tvalid_0's rmse: 1.82265\n",
    "#[600]\tvalid_0's rmse: 1.81725\n",
    "#[700]\tvalid_0's rmse: 1.81252\n",
    "#[800]\tvalid_0's rmse: 1.80736\n",
    "#[900]\tvalid_0's rmse: 1.80242\n",
    "#[1000]\tvalid_0's rmse: 1.79821\n",
    "#[1100]\tvalid_0's rmse: 1.794\n",
    "#[1200]\tvalid_0's rmse: 1.78973\n",
    "#[1300]\tvalid_0's rmse: 1.78552\n",
    "#[1400]\tvalid_0's rmse: 1.78158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "[100]\tvalid_0's rmse: 2.02752\n",
      "[200]\tvalid_0's rmse: 2.00461\n",
      "[300]\tvalid_0's rmse: 1.99412\n",
      "[400]\tvalid_0's rmse: 1.98743\n",
      "[500]\tvalid_0's rmse: 1.98155\n",
      "[600]\tvalid_0's rmse: 1.9761\n",
      "[700]\tvalid_0's rmse: 1.96979\n",
      "[800]\tvalid_0's rmse: 1.96483\n",
      "[900]\tvalid_0's rmse: 1.95973\n",
      "[1000]\tvalid_0's rmse: 1.9552\n",
      "[1100]\tvalid_0's rmse: 1.95001\n",
      "[1200]\tvalid_0's rmse: 1.94688\n",
      "[1300]\tvalid_0's rmse: 1.94244\n",
      "[1400]\tvalid_0's rmse: 1.93779\n",
      "Train CA_2\n",
      "[100]\tvalid_0's rmse: 1.89128\n",
      "[200]\tvalid_0's rmse: 1.85118\n",
      "[300]\tvalid_0's rmse: 1.84011\n",
      "[400]\tvalid_0's rmse: 1.83196\n",
      "[500]\tvalid_0's rmse: 1.82499\n",
      "[600]\tvalid_0's rmse: 1.81971\n",
      "[700]\tvalid_0's rmse: 1.81418\n",
      "[800]\tvalid_0's rmse: 1.80978\n",
      "[900]\tvalid_0's rmse: 1.80439\n",
      "[1000]\tvalid_0's rmse: 1.79982\n",
      "[1100]\tvalid_0's rmse: 1.79503\n",
      "[1200]\tvalid_0's rmse: 1.79078\n",
      "[1300]\tvalid_0's rmse: 1.78692\n",
      "[1400]\tvalid_0's rmse: 1.78329\n",
      "Train CA_3\n",
      "[100]\tvalid_0's rmse: 2.50702\n",
      "[200]\tvalid_0's rmse: 2.46213\n",
      "[300]\tvalid_0's rmse: 2.43955\n",
      "[400]\tvalid_0's rmse: 2.4272\n",
      "[500]\tvalid_0's rmse: 2.41807\n",
      "[600]\tvalid_0's rmse: 2.41112\n",
      "[700]\tvalid_0's rmse: 2.40462\n",
      "[800]\tvalid_0's rmse: 2.39885\n",
      "[900]\tvalid_0's rmse: 2.39183\n",
      "[1000]\tvalid_0's rmse: 2.38694\n",
      "[1100]\tvalid_0's rmse: 2.38295\n",
      "[1200]\tvalid_0's rmse: 2.37891\n",
      "[1300]\tvalid_0's rmse: 2.37374\n",
      "[1400]\tvalid_0's rmse: 2.36896\n",
      "Train CA_4\n",
      "[100]\tvalid_0's rmse: 1.33114\n",
      "[200]\tvalid_0's rmse: 1.3252\n",
      "[300]\tvalid_0's rmse: 1.3201\n",
      "[400]\tvalid_0's rmse: 1.31619\n",
      "[500]\tvalid_0's rmse: 1.31308\n",
      "[600]\tvalid_0's rmse: 1.30989\n",
      "[700]\tvalid_0's rmse: 1.30665\n",
      "[800]\tvalid_0's rmse: 1.30371\n",
      "[900]\tvalid_0's rmse: 1.30121\n",
      "[1000]\tvalid_0's rmse: 1.29859\n",
      "[1100]\tvalid_0's rmse: 1.29597\n",
      "[1200]\tvalid_0's rmse: 1.29368\n",
      "[1300]\tvalid_0's rmse: 1.29101\n",
      "[1400]\tvalid_0's rmse: 1.28839\n",
      "Train TX_1\n",
      "[100]\tvalid_0's rmse: 1.60679\n",
      "[200]\tvalid_0's rmse: 1.58679\n",
      "[300]\tvalid_0's rmse: 1.57896\n",
      "[400]\tvalid_0's rmse: 1.57375\n",
      "[500]\tvalid_0's rmse: 1.56909\n",
      "[600]\tvalid_0's rmse: 1.56517\n",
      "[700]\tvalid_0's rmse: 1.5617\n",
      "[800]\tvalid_0's rmse: 1.55821\n",
      "[900]\tvalid_0's rmse: 1.55459\n",
      "[1000]\tvalid_0's rmse: 1.55123\n",
      "[1100]\tvalid_0's rmse: 1.54746\n",
      "[1200]\tvalid_0's rmse: 1.54363\n",
      "[1300]\tvalid_0's rmse: 1.54044\n",
      "[1400]\tvalid_0's rmse: 1.5375\n",
      "Train TX_2\n",
      "[100]\tvalid_0's rmse: 1.7213\n",
      "[200]\tvalid_0's rmse: 1.70101\n",
      "[300]\tvalid_0's rmse: 1.68784\n",
      "[400]\tvalid_0's rmse: 1.68165\n",
      "[500]\tvalid_0's rmse: 1.67726\n",
      "[600]\tvalid_0's rmse: 1.67316\n",
      "[700]\tvalid_0's rmse: 1.66863\n",
      "[800]\tvalid_0's rmse: 1.66458\n",
      "[900]\tvalid_0's rmse: 1.66016\n",
      "[1000]\tvalid_0's rmse: 1.65701\n",
      "[1100]\tvalid_0's rmse: 1.65328\n",
      "[1200]\tvalid_0's rmse: 1.64952\n",
      "[1300]\tvalid_0's rmse: 1.6453\n",
      "[1400]\tvalid_0's rmse: 1.64175\n",
      "Train TX_3\n",
      "[100]\tvalid_0's rmse: 1.68752\n",
      "[200]\tvalid_0's rmse: 1.67611\n",
      "[300]\tvalid_0's rmse: 1.66966\n",
      "[400]\tvalid_0's rmse: 1.66474\n",
      "[500]\tvalid_0's rmse: 1.66103\n",
      "[600]\tvalid_0's rmse: 1.6552\n",
      "[700]\tvalid_0's rmse: 1.65092\n",
      "[800]\tvalid_0's rmse: 1.64596\n",
      "[900]\tvalid_0's rmse: 1.64215\n",
      "[1000]\tvalid_0's rmse: 1.63809\n",
      "[1100]\tvalid_0's rmse: 1.6341\n",
      "[1200]\tvalid_0's rmse: 1.63038\n",
      "[1300]\tvalid_0's rmse: 1.62653\n",
      "[1400]\tvalid_0's rmse: 1.62326\n",
      "Train WI_1\n",
      "[100]\tvalid_0's rmse: 1.5968\n",
      "[200]\tvalid_0's rmse: 1.5785\n",
      "[300]\tvalid_0's rmse: 1.5705\n",
      "[400]\tvalid_0's rmse: 1.56491\n",
      "[500]\tvalid_0's rmse: 1.56064\n",
      "[600]\tvalid_0's rmse: 1.55601\n",
      "[700]\tvalid_0's rmse: 1.55188\n",
      "[800]\tvalid_0's rmse: 1.54774\n",
      "[900]\tvalid_0's rmse: 1.54378\n",
      "[1000]\tvalid_0's rmse: 1.54025\n",
      "[1100]\tvalid_0's rmse: 1.53657\n",
      "[1200]\tvalid_0's rmse: 1.53327\n",
      "[1300]\tvalid_0's rmse: 1.52975\n",
      "[1400]\tvalid_0's rmse: 1.52647\n",
      "Train WI_2\n",
      "[100]\tvalid_0's rmse: 2.69025\n",
      "[200]\tvalid_0's rmse: 2.58637\n",
      "[300]\tvalid_0's rmse: 2.54793\n",
      "[400]\tvalid_0's rmse: 2.52462\n",
      "[500]\tvalid_0's rmse: 2.50765\n",
      "[600]\tvalid_0's rmse: 2.49214\n",
      "[700]\tvalid_0's rmse: 2.48179\n",
      "[800]\tvalid_0's rmse: 2.46981\n",
      "[900]\tvalid_0's rmse: 2.45856\n",
      "[1000]\tvalid_0's rmse: 2.4474\n",
      "[1100]\tvalid_0's rmse: 2.43839\n",
      "[1200]\tvalid_0's rmse: 2.4281\n",
      "[1300]\tvalid_0's rmse: 2.41993\n",
      "[1400]\tvalid_0's rmse: 2.40953\n",
      "Train WI_3\n",
      "[100]\tvalid_0's rmse: 1.91801\n",
      "[200]\tvalid_0's rmse: 1.85084\n",
      "[300]\tvalid_0's rmse: 1.82974\n",
      "[400]\tvalid_0's rmse: 1.81905\n",
      "[500]\tvalid_0's rmse: 1.80996\n",
      "[600]\tvalid_0's rmse: 1.80105\n",
      "[700]\tvalid_0's rmse: 1.7943\n",
      "[800]\tvalid_0's rmse: 1.78764\n",
      "[900]\tvalid_0's rmse: 1.78105\n",
      "[1000]\tvalid_0's rmse: 1.77666\n",
      "[1100]\tvalid_0's rmse: 1.77039\n",
      "[1200]\tvalid_0's rmse: 1.76569\n",
      "[1300]\tvalid_0's rmse: 1.76008\n",
      "[1400]\tvalid_0's rmse: 1.75532\n"
     ]
    }
   ],
   "source": [
    "########################### Train Models\n",
    "#################################################################################\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "    train_data.save_binary('train_data.bin')\n",
    "    train_data = lgb.Dataset('train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+store_id+'.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  0.45 min round |  0.45 min total |  37312.54 day sales |\n",
      "Predict | Day: 2\n",
      "##########  0.46 min round |  0.91 min total |  35225.87 day sales |\n",
      "Predict | Day: 3\n",
      "##########  0.46 min round |  1.37 min total |  34652.67 day sales |\n",
      "Predict | Day: 4\n",
      "##########  0.46 min round |  1.82 min total |  35237.68 day sales |\n",
      "Predict | Day: 5\n",
      "##########  0.46 min round |  2.28 min total |  41616.00 day sales |\n",
      "Predict | Day: 6\n",
      "##########  0.46 min round |  2.74 min total |  50872.86 day sales |\n",
      "Predict | Day: 7\n",
      "##########  0.46 min round |  3.20 min total |  53331.46 day sales |\n",
      "Predict | Day: 8\n",
      "##########  0.46 min round |  3.65 min total |  44045.62 day sales |\n",
      "Predict | Day: 9\n",
      "##########  0.46 min round |  4.11 min total |  44232.10 day sales |\n",
      "Predict | Day: 10\n",
      "##########  0.46 min round |  4.57 min total |  38627.41 day sales |\n",
      "Predict | Day: 11\n",
      "##########  0.45 min round |  5.02 min total |  40665.09 day sales |\n",
      "Predict | Day: 12\n",
      "##########  0.46 min round |  5.48 min total |  45825.32 day sales |\n",
      "Predict | Day: 13\n",
      "##########  0.46 min round |  5.94 min total |  53890.03 day sales |\n",
      "Predict | Day: 14\n",
      "##########  0.46 min round |  6.40 min total |  46271.95 day sales |\n",
      "Predict | Day: 15\n",
      "##########  0.46 min round |  6.86 min total |  44768.72 day sales |\n",
      "Predict | Day: 16\n",
      "##########  0.46 min round |  7.32 min total |  39361.93 day sales |\n",
      "Predict | Day: 17\n",
      "##########  0.46 min round |  7.78 min total |  40357.32 day sales |\n",
      "Predict | Day: 18\n",
      "##########  0.46 min round |  8.24 min total |  40911.35 day sales |\n",
      "Predict | Day: 19\n",
      "##########  0.46 min round |  8.69 min total |  43920.75 day sales |\n",
      "Predict | Day: 20\n",
      "##########  0.45 min round |  9.15 min total |  53570.59 day sales |\n",
      "Predict | Day: 21\n",
      "##########  0.46 min round |  9.61 min total |  55848.41 day sales |\n",
      "Predict | Day: 22\n",
      "##########  0.46 min round |  10.06 min total |  41760.27 day sales |\n",
      "Predict | Day: 23\n",
      "##########  0.45 min round |  10.52 min total |  37877.12 day sales |\n",
      "Predict | Day: 24\n",
      "##########  0.46 min round |  10.97 min total |  37064.21 day sales |\n",
      "Predict | Day: 25\n",
      "##########  0.45 min round |  11.43 min total |  37032.19 day sales |\n",
      "Predict | Day: 26\n",
      "##########  0.46 min round |  11.89 min total |  41916.69 day sales |\n",
      "Predict | Day: 27\n",
      "##########  0.46 min round |  12.35 min total |  50752.14 day sales |\n",
      "Predict | Day: 28\n",
      "##########  0.46 min round |  12.80 min total |  51338.34 day sales |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.817276</td>\n",
       "      <td>0.770136</td>\n",
       "      <td>0.773063</td>\n",
       "      <td>0.734750</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>1.159193</td>\n",
       "      <td>1.223346</td>\n",
       "      <td>0.918734</td>\n",
       "      <td>0.969244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779733</td>\n",
       "      <td>1.119184</td>\n",
       "      <td>1.027063</td>\n",
       "      <td>0.828853</td>\n",
       "      <td>0.806498</td>\n",
       "      <td>0.808823</td>\n",
       "      <td>0.790493</td>\n",
       "      <td>0.935414</td>\n",
       "      <td>1.129197</td>\n",
       "      <td>1.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.161672</td>\n",
       "      <td>0.157221</td>\n",
       "      <td>0.150143</td>\n",
       "      <td>0.152620</td>\n",
       "      <td>0.200712</td>\n",
       "      <td>0.261922</td>\n",
       "      <td>0.289153</td>\n",
       "      <td>0.209044</td>\n",
       "      <td>0.189601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197146</td>\n",
       "      <td>0.210441</td>\n",
       "      <td>0.214696</td>\n",
       "      <td>0.174457</td>\n",
       "      <td>0.162774</td>\n",
       "      <td>0.171702</td>\n",
       "      <td>0.177484</td>\n",
       "      <td>0.178244</td>\n",
       "      <td>0.250040</td>\n",
       "      <td>0.239309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.431134</td>\n",
       "      <td>0.406738</td>\n",
       "      <td>0.424826</td>\n",
       "      <td>0.435743</td>\n",
       "      <td>0.651485</td>\n",
       "      <td>0.875942</td>\n",
       "      <td>0.672834</td>\n",
       "      <td>0.480412</td>\n",
       "      <td>0.487221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509719</td>\n",
       "      <td>0.642228</td>\n",
       "      <td>0.673082</td>\n",
       "      <td>0.491763</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>0.430336</td>\n",
       "      <td>0.463548</td>\n",
       "      <td>0.586319</td>\n",
       "      <td>0.726258</td>\n",
       "      <td>0.719822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.677102</td>\n",
       "      <td>1.399713</td>\n",
       "      <td>1.320517</td>\n",
       "      <td>1.471098</td>\n",
       "      <td>1.804805</td>\n",
       "      <td>3.016687</td>\n",
       "      <td>2.998087</td>\n",
       "      <td>1.693713</td>\n",
       "      <td>1.477424</td>\n",
       "      <td>...</td>\n",
       "      <td>1.816084</td>\n",
       "      <td>2.601878</td>\n",
       "      <td>3.209413</td>\n",
       "      <td>1.592659</td>\n",
       "      <td>1.483907</td>\n",
       "      <td>1.377427</td>\n",
       "      <td>1.345501</td>\n",
       "      <td>1.821208</td>\n",
       "      <td>2.918769</td>\n",
       "      <td>3.293197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.926041</td>\n",
       "      <td>0.812809</td>\n",
       "      <td>0.782949</td>\n",
       "      <td>0.850560</td>\n",
       "      <td>1.026819</td>\n",
       "      <td>1.428682</td>\n",
       "      <td>1.411533</td>\n",
       "      <td>0.992563</td>\n",
       "      <td>1.041681</td>\n",
       "      <td>...</td>\n",
       "      <td>1.042151</td>\n",
       "      <td>1.459160</td>\n",
       "      <td>1.587306</td>\n",
       "      <td>0.987382</td>\n",
       "      <td>0.855638</td>\n",
       "      <td>0.884560</td>\n",
       "      <td>0.859523</td>\n",
       "      <td>1.088437</td>\n",
       "      <td>1.383876</td>\n",
       "      <td>1.465852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HOBBIES_1_006_CA_1_validation</td>\n",
       "      <td>0.804261</td>\n",
       "      <td>0.783431</td>\n",
       "      <td>0.774554</td>\n",
       "      <td>0.824596</td>\n",
       "      <td>0.855111</td>\n",
       "      <td>0.947720</td>\n",
       "      <td>1.094119</td>\n",
       "      <td>0.888501</td>\n",
       "      <td>0.815705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686038</td>\n",
       "      <td>0.934931</td>\n",
       "      <td>0.931819</td>\n",
       "      <td>0.803367</td>\n",
       "      <td>0.686886</td>\n",
       "      <td>0.749135</td>\n",
       "      <td>0.756473</td>\n",
       "      <td>0.733492</td>\n",
       "      <td>0.863749</td>\n",
       "      <td>0.850394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOBBIES_1_007_CA_1_validation</td>\n",
       "      <td>0.296339</td>\n",
       "      <td>0.255662</td>\n",
       "      <td>0.265615</td>\n",
       "      <td>0.272955</td>\n",
       "      <td>0.296285</td>\n",
       "      <td>0.348281</td>\n",
       "      <td>0.419274</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333472</td>\n",
       "      <td>0.394438</td>\n",
       "      <td>0.474240</td>\n",
       "      <td>0.331565</td>\n",
       "      <td>0.349792</td>\n",
       "      <td>0.320193</td>\n",
       "      <td>0.307124</td>\n",
       "      <td>0.313301</td>\n",
       "      <td>0.412904</td>\n",
       "      <td>0.414125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>6.330922</td>\n",
       "      <td>7.418705</td>\n",
       "      <td>8.388321</td>\n",
       "      <td>7.513185</td>\n",
       "      <td>8.782354</td>\n",
       "      <td>9.019364</td>\n",
       "      <td>7.580533</td>\n",
       "      <td>8.519221</td>\n",
       "      <td>7.916029</td>\n",
       "      <td>...</td>\n",
       "      <td>8.001297</td>\n",
       "      <td>9.079751</td>\n",
       "      <td>8.545755</td>\n",
       "      <td>7.654284</td>\n",
       "      <td>7.249439</td>\n",
       "      <td>7.254823</td>\n",
       "      <td>8.063553</td>\n",
       "      <td>7.566560</td>\n",
       "      <td>8.891785</td>\n",
       "      <td>6.633204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>0.842939</td>\n",
       "      <td>0.914360</td>\n",
       "      <td>0.793283</td>\n",
       "      <td>0.892036</td>\n",
       "      <td>0.997161</td>\n",
       "      <td>1.108806</td>\n",
       "      <td>1.337384</td>\n",
       "      <td>0.955499</td>\n",
       "      <td>0.907876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888622</td>\n",
       "      <td>1.143173</td>\n",
       "      <td>1.278639</td>\n",
       "      <td>0.941930</td>\n",
       "      <td>0.985323</td>\n",
       "      <td>0.851697</td>\n",
       "      <td>0.859277</td>\n",
       "      <td>0.853214</td>\n",
       "      <td>1.136018</td>\n",
       "      <td>1.139906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>0.600570</td>\n",
       "      <td>0.476667</td>\n",
       "      <td>0.493015</td>\n",
       "      <td>0.470509</td>\n",
       "      <td>0.573394</td>\n",
       "      <td>0.832895</td>\n",
       "      <td>0.915917</td>\n",
       "      <td>0.640671</td>\n",
       "      <td>0.665931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.576035</td>\n",
       "      <td>0.717115</td>\n",
       "      <td>0.805349</td>\n",
       "      <td>0.592424</td>\n",
       "      <td>0.516743</td>\n",
       "      <td>0.528799</td>\n",
       "      <td>0.566638</td>\n",
       "      <td>0.657165</td>\n",
       "      <td>0.763480</td>\n",
       "      <td>0.838685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOBBIES_1_011_CA_1_validation</td>\n",
       "      <td>0.074993</td>\n",
       "      <td>0.087449</td>\n",
       "      <td>0.086982</td>\n",
       "      <td>0.092422</td>\n",
       "      <td>0.110987</td>\n",
       "      <td>0.134317</td>\n",
       "      <td>0.134498</td>\n",
       "      <td>0.082918</td>\n",
       "      <td>0.079890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114705</td>\n",
       "      <td>0.168386</td>\n",
       "      <td>0.175080</td>\n",
       "      <td>0.096131</td>\n",
       "      <td>0.097127</td>\n",
       "      <td>0.109733</td>\n",
       "      <td>0.124072</td>\n",
       "      <td>0.143694</td>\n",
       "      <td>0.163460</td>\n",
       "      <td>0.171485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>0.181734</td>\n",
       "      <td>0.193036</td>\n",
       "      <td>0.179694</td>\n",
       "      <td>0.199297</td>\n",
       "      <td>0.221838</td>\n",
       "      <td>0.324098</td>\n",
       "      <td>0.309741</td>\n",
       "      <td>0.198635</td>\n",
       "      <td>0.186451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191741</td>\n",
       "      <td>0.311125</td>\n",
       "      <td>0.400580</td>\n",
       "      <td>0.210034</td>\n",
       "      <td>0.179597</td>\n",
       "      <td>0.187246</td>\n",
       "      <td>0.217563</td>\n",
       "      <td>0.251681</td>\n",
       "      <td>0.359535</td>\n",
       "      <td>0.371984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HOBBIES_1_013_CA_1_validation</td>\n",
       "      <td>0.312128</td>\n",
       "      <td>0.287521</td>\n",
       "      <td>0.279929</td>\n",
       "      <td>0.302888</td>\n",
       "      <td>0.394982</td>\n",
       "      <td>0.508918</td>\n",
       "      <td>0.515574</td>\n",
       "      <td>0.321108</td>\n",
       "      <td>0.311120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331940</td>\n",
       "      <td>0.572990</td>\n",
       "      <td>0.595005</td>\n",
       "      <td>0.390757</td>\n",
       "      <td>0.321206</td>\n",
       "      <td>0.340255</td>\n",
       "      <td>0.296404</td>\n",
       "      <td>0.391233</td>\n",
       "      <td>0.535591</td>\n",
       "      <td>0.591458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HOBBIES_1_014_CA_1_validation</td>\n",
       "      <td>1.680243</td>\n",
       "      <td>1.583166</td>\n",
       "      <td>1.530672</td>\n",
       "      <td>1.526282</td>\n",
       "      <td>1.733708</td>\n",
       "      <td>2.194308</td>\n",
       "      <td>2.212691</td>\n",
       "      <td>1.804338</td>\n",
       "      <td>1.647237</td>\n",
       "      <td>...</td>\n",
       "      <td>1.594097</td>\n",
       "      <td>1.832842</td>\n",
       "      <td>1.843334</td>\n",
       "      <td>1.540705</td>\n",
       "      <td>1.404503</td>\n",
       "      <td>1.569764</td>\n",
       "      <td>1.655459</td>\n",
       "      <td>1.711671</td>\n",
       "      <td>1.929680</td>\n",
       "      <td>1.773803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>3.196302</td>\n",
       "      <td>3.077121</td>\n",
       "      <td>3.251699</td>\n",
       "      <td>3.227031</td>\n",
       "      <td>3.771418</td>\n",
       "      <td>5.000935</td>\n",
       "      <td>4.747428</td>\n",
       "      <td>3.339115</td>\n",
       "      <td>3.069590</td>\n",
       "      <td>...</td>\n",
       "      <td>3.476131</td>\n",
       "      <td>4.986322</td>\n",
       "      <td>4.489625</td>\n",
       "      <td>2.721986</td>\n",
       "      <td>2.780687</td>\n",
       "      <td>2.698921</td>\n",
       "      <td>2.988750</td>\n",
       "      <td>3.733684</td>\n",
       "      <td>5.115591</td>\n",
       "      <td>4.831274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>5.766064</td>\n",
       "      <td>5.459336</td>\n",
       "      <td>5.048143</td>\n",
       "      <td>5.135166</td>\n",
       "      <td>5.468039</td>\n",
       "      <td>8.005105</td>\n",
       "      <td>6.202049</td>\n",
       "      <td>5.693248</td>\n",
       "      <td>5.160178</td>\n",
       "      <td>...</td>\n",
       "      <td>5.775569</td>\n",
       "      <td>7.974805</td>\n",
       "      <td>7.422209</td>\n",
       "      <td>5.467073</td>\n",
       "      <td>4.825719</td>\n",
       "      <td>5.435774</td>\n",
       "      <td>5.282380</td>\n",
       "      <td>6.168832</td>\n",
       "      <td>7.690149</td>\n",
       "      <td>6.848590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HOBBIES_1_017_CA_1_validation</td>\n",
       "      <td>0.945009</td>\n",
       "      <td>0.896739</td>\n",
       "      <td>0.830838</td>\n",
       "      <td>0.881532</td>\n",
       "      <td>1.195535</td>\n",
       "      <td>1.659318</td>\n",
       "      <td>1.842948</td>\n",
       "      <td>1.051945</td>\n",
       "      <td>0.973063</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246429</td>\n",
       "      <td>1.519458</td>\n",
       "      <td>1.665724</td>\n",
       "      <td>0.942445</td>\n",
       "      <td>0.937349</td>\n",
       "      <td>0.989703</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>1.342874</td>\n",
       "      <td>1.906462</td>\n",
       "      <td>1.798525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HOBBIES_1_018_CA_1_validation</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>0.060461</td>\n",
       "      <td>0.057891</td>\n",
       "      <td>0.058446</td>\n",
       "      <td>0.072867</td>\n",
       "      <td>0.093913</td>\n",
       "      <td>0.094260</td>\n",
       "      <td>0.072050</td>\n",
       "      <td>0.066704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093462</td>\n",
       "      <td>0.111386</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>0.087953</td>\n",
       "      <td>0.081231</td>\n",
       "      <td>0.085177</td>\n",
       "      <td>0.086814</td>\n",
       "      <td>0.110446</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>0.131617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>HOBBIES_1_019_CA_1_validation</td>\n",
       "      <td>7.361619</td>\n",
       "      <td>6.423784</td>\n",
       "      <td>6.686461</td>\n",
       "      <td>7.105886</td>\n",
       "      <td>8.397991</td>\n",
       "      <td>8.518142</td>\n",
       "      <td>7.844825</td>\n",
       "      <td>8.096612</td>\n",
       "      <td>8.001851</td>\n",
       "      <td>...</td>\n",
       "      <td>7.447989</td>\n",
       "      <td>8.489655</td>\n",
       "      <td>7.651754</td>\n",
       "      <td>6.689675</td>\n",
       "      <td>5.948963</td>\n",
       "      <td>6.589930</td>\n",
       "      <td>7.367831</td>\n",
       "      <td>8.445051</td>\n",
       "      <td>8.304362</td>\n",
       "      <td>8.762527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HOBBIES_1_020_CA_1_validation</td>\n",
       "      <td>0.261856</td>\n",
       "      <td>0.252665</td>\n",
       "      <td>0.247014</td>\n",
       "      <td>0.256279</td>\n",
       "      <td>0.276505</td>\n",
       "      <td>0.316110</td>\n",
       "      <td>0.313629</td>\n",
       "      <td>0.245563</td>\n",
       "      <td>0.230483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270053</td>\n",
       "      <td>0.300276</td>\n",
       "      <td>0.304994</td>\n",
       "      <td>0.257892</td>\n",
       "      <td>0.263702</td>\n",
       "      <td>0.253771</td>\n",
       "      <td>0.259087</td>\n",
       "      <td>0.296674</td>\n",
       "      <td>0.375430</td>\n",
       "      <td>0.390166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HOBBIES_1_021_CA_1_validation</td>\n",
       "      <td>0.580208</td>\n",
       "      <td>0.572484</td>\n",
       "      <td>0.571020</td>\n",
       "      <td>0.655592</td>\n",
       "      <td>0.727170</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>0.827396</td>\n",
       "      <td>0.605667</td>\n",
       "      <td>0.567853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817640</td>\n",
       "      <td>0.866398</td>\n",
       "      <td>0.890955</td>\n",
       "      <td>0.660190</td>\n",
       "      <td>0.631043</td>\n",
       "      <td>0.642046</td>\n",
       "      <td>0.630578</td>\n",
       "      <td>0.737626</td>\n",
       "      <td>0.973417</td>\n",
       "      <td>0.961644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HOBBIES_1_022_CA_1_validation</td>\n",
       "      <td>0.356922</td>\n",
       "      <td>0.318589</td>\n",
       "      <td>0.275012</td>\n",
       "      <td>0.294085</td>\n",
       "      <td>0.418820</td>\n",
       "      <td>0.475940</td>\n",
       "      <td>0.528208</td>\n",
       "      <td>0.317346</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310080</td>\n",
       "      <td>0.525231</td>\n",
       "      <td>0.556916</td>\n",
       "      <td>0.348454</td>\n",
       "      <td>0.303816</td>\n",
       "      <td>0.290070</td>\n",
       "      <td>0.279553</td>\n",
       "      <td>0.346886</td>\n",
       "      <td>0.487306</td>\n",
       "      <td>0.508503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HOBBIES_1_023_CA_1_validation</td>\n",
       "      <td>1.418794</td>\n",
       "      <td>1.230790</td>\n",
       "      <td>1.146241</td>\n",
       "      <td>1.258297</td>\n",
       "      <td>1.597155</td>\n",
       "      <td>2.038093</td>\n",
       "      <td>2.113230</td>\n",
       "      <td>1.560609</td>\n",
       "      <td>1.330694</td>\n",
       "      <td>...</td>\n",
       "      <td>1.630253</td>\n",
       "      <td>2.187635</td>\n",
       "      <td>2.182459</td>\n",
       "      <td>1.581973</td>\n",
       "      <td>1.193530</td>\n",
       "      <td>1.185981</td>\n",
       "      <td>1.189071</td>\n",
       "      <td>1.613514</td>\n",
       "      <td>2.247573</td>\n",
       "      <td>2.139162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HOBBIES_1_024_CA_1_validation</td>\n",
       "      <td>0.222003</td>\n",
       "      <td>0.205209</td>\n",
       "      <td>0.196758</td>\n",
       "      <td>0.180647</td>\n",
       "      <td>0.249229</td>\n",
       "      <td>0.305855</td>\n",
       "      <td>0.287218</td>\n",
       "      <td>0.231260</td>\n",
       "      <td>0.213429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233391</td>\n",
       "      <td>0.317032</td>\n",
       "      <td>0.339943</td>\n",
       "      <td>0.230085</td>\n",
       "      <td>0.202754</td>\n",
       "      <td>0.202407</td>\n",
       "      <td>0.192086</td>\n",
       "      <td>0.275916</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.344243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>HOBBIES_1_025_CA_1_validation</td>\n",
       "      <td>0.483823</td>\n",
       "      <td>0.420764</td>\n",
       "      <td>0.446002</td>\n",
       "      <td>0.498881</td>\n",
       "      <td>0.677113</td>\n",
       "      <td>0.854955</td>\n",
       "      <td>1.003496</td>\n",
       "      <td>0.467169</td>\n",
       "      <td>0.471854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636575</td>\n",
       "      <td>0.792754</td>\n",
       "      <td>0.995948</td>\n",
       "      <td>0.494750</td>\n",
       "      <td>0.443552</td>\n",
       "      <td>0.429469</td>\n",
       "      <td>0.424702</td>\n",
       "      <td>0.605102</td>\n",
       "      <td>0.796178</td>\n",
       "      <td>0.862890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>HOBBIES_1_026_CA_1_validation</td>\n",
       "      <td>0.191684</td>\n",
       "      <td>0.172332</td>\n",
       "      <td>0.177754</td>\n",
       "      <td>0.202233</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>0.272854</td>\n",
       "      <td>0.322485</td>\n",
       "      <td>0.217753</td>\n",
       "      <td>0.199495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193005</td>\n",
       "      <td>0.246253</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>0.194431</td>\n",
       "      <td>0.184959</td>\n",
       "      <td>0.171535</td>\n",
       "      <td>0.170053</td>\n",
       "      <td>0.214665</td>\n",
       "      <td>0.264125</td>\n",
       "      <td>0.268291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HOBBIES_1_027_CA_1_validation</td>\n",
       "      <td>0.351637</td>\n",
       "      <td>0.324101</td>\n",
       "      <td>0.321241</td>\n",
       "      <td>0.314376</td>\n",
       "      <td>0.404987</td>\n",
       "      <td>0.641048</td>\n",
       "      <td>0.646389</td>\n",
       "      <td>0.361954</td>\n",
       "      <td>0.351747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411727</td>\n",
       "      <td>0.546008</td>\n",
       "      <td>0.561487</td>\n",
       "      <td>0.337373</td>\n",
       "      <td>0.350231</td>\n",
       "      <td>0.308079</td>\n",
       "      <td>0.320112</td>\n",
       "      <td>0.396613</td>\n",
       "      <td>0.510721</td>\n",
       "      <td>0.513784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HOBBIES_1_028_CA_1_validation</td>\n",
       "      <td>0.561631</td>\n",
       "      <td>0.517261</td>\n",
       "      <td>0.536507</td>\n",
       "      <td>0.595838</td>\n",
       "      <td>0.713343</td>\n",
       "      <td>0.880129</td>\n",
       "      <td>0.969188</td>\n",
       "      <td>0.755633</td>\n",
       "      <td>0.710308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786059</td>\n",
       "      <td>0.817497</td>\n",
       "      <td>0.812331</td>\n",
       "      <td>0.676202</td>\n",
       "      <td>0.574158</td>\n",
       "      <td>0.596210</td>\n",
       "      <td>0.676081</td>\n",
       "      <td>0.744756</td>\n",
       "      <td>0.907394</td>\n",
       "      <td>0.801126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HOBBIES_1_029_CA_1_validation</td>\n",
       "      <td>1.592014</td>\n",
       "      <td>1.366301</td>\n",
       "      <td>1.299063</td>\n",
       "      <td>1.352318</td>\n",
       "      <td>1.620083</td>\n",
       "      <td>2.093550</td>\n",
       "      <td>2.096189</td>\n",
       "      <td>1.491071</td>\n",
       "      <td>1.453033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.741985</td>\n",
       "      <td>2.187173</td>\n",
       "      <td>2.212590</td>\n",
       "      <td>1.533194</td>\n",
       "      <td>1.406845</td>\n",
       "      <td>1.277220</td>\n",
       "      <td>1.243139</td>\n",
       "      <td>1.707174</td>\n",
       "      <td>1.945735</td>\n",
       "      <td>2.089233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>HOBBIES_1_030_CA_1_validation</td>\n",
       "      <td>5.292437</td>\n",
       "      <td>4.296557</td>\n",
       "      <td>4.136964</td>\n",
       "      <td>4.139414</td>\n",
       "      <td>6.221961</td>\n",
       "      <td>7.883964</td>\n",
       "      <td>7.882635</td>\n",
       "      <td>5.616298</td>\n",
       "      <td>5.560988</td>\n",
       "      <td>...</td>\n",
       "      <td>6.563665</td>\n",
       "      <td>9.084987</td>\n",
       "      <td>8.476809</td>\n",
       "      <td>5.431188</td>\n",
       "      <td>4.647069</td>\n",
       "      <td>4.935142</td>\n",
       "      <td>4.568084</td>\n",
       "      <td>6.173704</td>\n",
       "      <td>8.576298</td>\n",
       "      <td>8.214426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30460</th>\n",
       "      <td>FOODS_3_798_WI_3_validation</td>\n",
       "      <td>0.321229</td>\n",
       "      <td>0.304133</td>\n",
       "      <td>0.293710</td>\n",
       "      <td>0.286341</td>\n",
       "      <td>0.394677</td>\n",
       "      <td>0.505779</td>\n",
       "      <td>0.390127</td>\n",
       "      <td>0.406330</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451224</td>\n",
       "      <td>0.550685</td>\n",
       "      <td>0.567712</td>\n",
       "      <td>0.383665</td>\n",
       "      <td>0.355652</td>\n",
       "      <td>0.339711</td>\n",
       "      <td>0.330426</td>\n",
       "      <td>0.448856</td>\n",
       "      <td>0.532729</td>\n",
       "      <td>0.498438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30461</th>\n",
       "      <td>FOODS_3_799_WI_3_validation</td>\n",
       "      <td>0.170079</td>\n",
       "      <td>0.178261</td>\n",
       "      <td>0.163283</td>\n",
       "      <td>0.178484</td>\n",
       "      <td>0.206090</td>\n",
       "      <td>0.262792</td>\n",
       "      <td>0.236955</td>\n",
       "      <td>0.279629</td>\n",
       "      <td>0.302050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296256</td>\n",
       "      <td>0.343787</td>\n",
       "      <td>0.316611</td>\n",
       "      <td>0.268926</td>\n",
       "      <td>0.275248</td>\n",
       "      <td>0.263621</td>\n",
       "      <td>0.211299</td>\n",
       "      <td>0.203877</td>\n",
       "      <td>0.264354</td>\n",
       "      <td>0.254505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30462</th>\n",
       "      <td>FOODS_3_800_WI_3_validation</td>\n",
       "      <td>6.282155</td>\n",
       "      <td>6.001415</td>\n",
       "      <td>5.909923</td>\n",
       "      <td>6.044600</td>\n",
       "      <td>7.848129</td>\n",
       "      <td>10.047003</td>\n",
       "      <td>9.541103</td>\n",
       "      <td>10.591332</td>\n",
       "      <td>10.293884</td>\n",
       "      <td>...</td>\n",
       "      <td>8.918664</td>\n",
       "      <td>11.824297</td>\n",
       "      <td>12.799487</td>\n",
       "      <td>8.290161</td>\n",
       "      <td>7.288364</td>\n",
       "      <td>6.714998</td>\n",
       "      <td>7.096859</td>\n",
       "      <td>8.260453</td>\n",
       "      <td>9.861054</td>\n",
       "      <td>9.713196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30463</th>\n",
       "      <td>FOODS_3_801_WI_3_validation</td>\n",
       "      <td>2.274663</td>\n",
       "      <td>2.107307</td>\n",
       "      <td>1.857913</td>\n",
       "      <td>1.979711</td>\n",
       "      <td>2.398504</td>\n",
       "      <td>3.319686</td>\n",
       "      <td>2.818320</td>\n",
       "      <td>3.292629</td>\n",
       "      <td>3.500330</td>\n",
       "      <td>...</td>\n",
       "      <td>2.873259</td>\n",
       "      <td>3.705753</td>\n",
       "      <td>3.671871</td>\n",
       "      <td>2.524961</td>\n",
       "      <td>2.316742</td>\n",
       "      <td>2.257988</td>\n",
       "      <td>2.257700</td>\n",
       "      <td>2.801325</td>\n",
       "      <td>2.734822</td>\n",
       "      <td>2.745616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30464</th>\n",
       "      <td>FOODS_3_802_WI_3_validation</td>\n",
       "      <td>0.231184</td>\n",
       "      <td>0.229869</td>\n",
       "      <td>0.222126</td>\n",
       "      <td>0.263466</td>\n",
       "      <td>0.384435</td>\n",
       "      <td>0.496687</td>\n",
       "      <td>0.432486</td>\n",
       "      <td>0.442373</td>\n",
       "      <td>0.488369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505850</td>\n",
       "      <td>0.610420</td>\n",
       "      <td>0.710522</td>\n",
       "      <td>0.386773</td>\n",
       "      <td>0.329013</td>\n",
       "      <td>0.304977</td>\n",
       "      <td>0.273506</td>\n",
       "      <td>0.375846</td>\n",
       "      <td>0.464383</td>\n",
       "      <td>0.441479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30465</th>\n",
       "      <td>FOODS_3_803_WI_3_validation</td>\n",
       "      <td>0.094647</td>\n",
       "      <td>0.797766</td>\n",
       "      <td>0.822766</td>\n",
       "      <td>1.295276</td>\n",
       "      <td>1.515854</td>\n",
       "      <td>1.553098</td>\n",
       "      <td>1.229574</td>\n",
       "      <td>1.209004</td>\n",
       "      <td>1.298607</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331270</td>\n",
       "      <td>1.607535</td>\n",
       "      <td>1.546699</td>\n",
       "      <td>1.079257</td>\n",
       "      <td>1.089646</td>\n",
       "      <td>1.030142</td>\n",
       "      <td>1.032716</td>\n",
       "      <td>1.227434</td>\n",
       "      <td>1.432852</td>\n",
       "      <td>1.304518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30466</th>\n",
       "      <td>FOODS_3_804_WI_3_validation</td>\n",
       "      <td>4.411156</td>\n",
       "      <td>4.414146</td>\n",
       "      <td>4.288328</td>\n",
       "      <td>3.976432</td>\n",
       "      <td>5.076610</td>\n",
       "      <td>6.495052</td>\n",
       "      <td>6.284872</td>\n",
       "      <td>7.541868</td>\n",
       "      <td>7.617856</td>\n",
       "      <td>...</td>\n",
       "      <td>6.931394</td>\n",
       "      <td>9.473998</td>\n",
       "      <td>8.717522</td>\n",
       "      <td>6.552831</td>\n",
       "      <td>5.874569</td>\n",
       "      <td>5.816035</td>\n",
       "      <td>6.077403</td>\n",
       "      <td>6.990247</td>\n",
       "      <td>7.856869</td>\n",
       "      <td>7.030278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30467</th>\n",
       "      <td>FOODS_3_805_WI_3_validation</td>\n",
       "      <td>1.507966</td>\n",
       "      <td>1.391781</td>\n",
       "      <td>1.192361</td>\n",
       "      <td>1.335825</td>\n",
       "      <td>1.633870</td>\n",
       "      <td>2.131017</td>\n",
       "      <td>2.020013</td>\n",
       "      <td>2.185843</td>\n",
       "      <td>2.348764</td>\n",
       "      <td>...</td>\n",
       "      <td>1.701333</td>\n",
       "      <td>2.518507</td>\n",
       "      <td>2.317897</td>\n",
       "      <td>1.729988</td>\n",
       "      <td>1.552497</td>\n",
       "      <td>1.454900</td>\n",
       "      <td>1.449240</td>\n",
       "      <td>1.712414</td>\n",
       "      <td>1.855091</td>\n",
       "      <td>1.782754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30468</th>\n",
       "      <td>FOODS_3_806_WI_3_validation</td>\n",
       "      <td>0.563711</td>\n",
       "      <td>0.503447</td>\n",
       "      <td>0.432227</td>\n",
       "      <td>0.432874</td>\n",
       "      <td>0.515128</td>\n",
       "      <td>0.598674</td>\n",
       "      <td>0.660179</td>\n",
       "      <td>0.669177</td>\n",
       "      <td>0.650455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600637</td>\n",
       "      <td>0.832547</td>\n",
       "      <td>0.862744</td>\n",
       "      <td>0.566447</td>\n",
       "      <td>0.552940</td>\n",
       "      <td>0.565051</td>\n",
       "      <td>0.517177</td>\n",
       "      <td>0.533310</td>\n",
       "      <td>0.662704</td>\n",
       "      <td>0.617546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30469</th>\n",
       "      <td>FOODS_3_807_WI_3_validation</td>\n",
       "      <td>2.546803</td>\n",
       "      <td>2.316860</td>\n",
       "      <td>1.827463</td>\n",
       "      <td>1.955103</td>\n",
       "      <td>2.535162</td>\n",
       "      <td>3.058748</td>\n",
       "      <td>2.764864</td>\n",
       "      <td>3.554619</td>\n",
       "      <td>3.881389</td>\n",
       "      <td>...</td>\n",
       "      <td>2.955379</td>\n",
       "      <td>4.169486</td>\n",
       "      <td>4.374120</td>\n",
       "      <td>3.313321</td>\n",
       "      <td>2.879224</td>\n",
       "      <td>2.566886</td>\n",
       "      <td>2.612659</td>\n",
       "      <td>2.707073</td>\n",
       "      <td>3.277590</td>\n",
       "      <td>3.267757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30470</th>\n",
       "      <td>FOODS_3_808_WI_3_validation</td>\n",
       "      <td>0.200325</td>\n",
       "      <td>1.743984</td>\n",
       "      <td>3.175181</td>\n",
       "      <td>5.354431</td>\n",
       "      <td>8.096684</td>\n",
       "      <td>14.209224</td>\n",
       "      <td>19.560379</td>\n",
       "      <td>21.738900</td>\n",
       "      <td>18.476413</td>\n",
       "      <td>...</td>\n",
       "      <td>14.194121</td>\n",
       "      <td>19.526597</td>\n",
       "      <td>21.260601</td>\n",
       "      <td>15.684528</td>\n",
       "      <td>13.047093</td>\n",
       "      <td>10.947351</td>\n",
       "      <td>10.220070</td>\n",
       "      <td>11.146726</td>\n",
       "      <td>14.446118</td>\n",
       "      <td>14.842901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30471</th>\n",
       "      <td>FOODS_3_809_WI_3_validation</td>\n",
       "      <td>1.851644</td>\n",
       "      <td>1.760576</td>\n",
       "      <td>1.749796</td>\n",
       "      <td>1.792445</td>\n",
       "      <td>2.400186</td>\n",
       "      <td>2.514697</td>\n",
       "      <td>2.510362</td>\n",
       "      <td>3.382664</td>\n",
       "      <td>3.231788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.372498</td>\n",
       "      <td>2.983672</td>\n",
       "      <td>2.765355</td>\n",
       "      <td>2.482787</td>\n",
       "      <td>2.084702</td>\n",
       "      <td>1.829149</td>\n",
       "      <td>1.901561</td>\n",
       "      <td>2.303145</td>\n",
       "      <td>2.321571</td>\n",
       "      <td>1.953122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30472</th>\n",
       "      <td>FOODS_3_810_WI_3_validation</td>\n",
       "      <td>0.631353</td>\n",
       "      <td>0.618864</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.502486</td>\n",
       "      <td>0.667869</td>\n",
       "      <td>0.814034</td>\n",
       "      <td>0.670948</td>\n",
       "      <td>0.648460</td>\n",
       "      <td>0.643499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660334</td>\n",
       "      <td>0.756267</td>\n",
       "      <td>0.805342</td>\n",
       "      <td>0.641139</td>\n",
       "      <td>0.603470</td>\n",
       "      <td>0.597127</td>\n",
       "      <td>0.554197</td>\n",
       "      <td>0.639804</td>\n",
       "      <td>0.782455</td>\n",
       "      <td>0.770890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30473</th>\n",
       "      <td>FOODS_3_811_WI_3_validation</td>\n",
       "      <td>15.629869</td>\n",
       "      <td>14.836148</td>\n",
       "      <td>12.865305</td>\n",
       "      <td>12.714345</td>\n",
       "      <td>14.129779</td>\n",
       "      <td>18.429221</td>\n",
       "      <td>18.017380</td>\n",
       "      <td>20.270525</td>\n",
       "      <td>20.240907</td>\n",
       "      <td>...</td>\n",
       "      <td>16.130130</td>\n",
       "      <td>22.199482</td>\n",
       "      <td>22.514359</td>\n",
       "      <td>14.233854</td>\n",
       "      <td>13.936478</td>\n",
       "      <td>12.075680</td>\n",
       "      <td>13.063460</td>\n",
       "      <td>13.865376</td>\n",
       "      <td>15.717298</td>\n",
       "      <td>14.278867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30474</th>\n",
       "      <td>FOODS_3_812_WI_3_validation</td>\n",
       "      <td>1.034687</td>\n",
       "      <td>0.925409</td>\n",
       "      <td>0.942585</td>\n",
       "      <td>0.944759</td>\n",
       "      <td>1.263260</td>\n",
       "      <td>1.401484</td>\n",
       "      <td>1.456685</td>\n",
       "      <td>1.826407</td>\n",
       "      <td>1.740029</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511094</td>\n",
       "      <td>1.858865</td>\n",
       "      <td>2.078866</td>\n",
       "      <td>1.176930</td>\n",
       "      <td>1.036719</td>\n",
       "      <td>1.075472</td>\n",
       "      <td>1.013438</td>\n",
       "      <td>1.176367</td>\n",
       "      <td>1.416324</td>\n",
       "      <td>1.449187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30475</th>\n",
       "      <td>FOODS_3_813_WI_3_validation</td>\n",
       "      <td>0.289267</td>\n",
       "      <td>0.271676</td>\n",
       "      <td>0.234282</td>\n",
       "      <td>0.229281</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.247041</td>\n",
       "      <td>0.211932</td>\n",
       "      <td>0.338137</td>\n",
       "      <td>0.358633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>0.343628</td>\n",
       "      <td>0.373080</td>\n",
       "      <td>0.355992</td>\n",
       "      <td>0.334263</td>\n",
       "      <td>0.315004</td>\n",
       "      <td>0.286013</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.289749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30476</th>\n",
       "      <td>FOODS_3_814_WI_3_validation</td>\n",
       "      <td>1.741611</td>\n",
       "      <td>1.713006</td>\n",
       "      <td>1.713718</td>\n",
       "      <td>1.778721</td>\n",
       "      <td>2.174868</td>\n",
       "      <td>2.412760</td>\n",
       "      <td>2.265626</td>\n",
       "      <td>2.407542</td>\n",
       "      <td>2.371163</td>\n",
       "      <td>...</td>\n",
       "      <td>2.206821</td>\n",
       "      <td>2.749198</td>\n",
       "      <td>2.806340</td>\n",
       "      <td>1.978183</td>\n",
       "      <td>1.752176</td>\n",
       "      <td>1.724408</td>\n",
       "      <td>1.711915</td>\n",
       "      <td>2.020434</td>\n",
       "      <td>2.394021</td>\n",
       "      <td>2.241675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30477</th>\n",
       "      <td>FOODS_3_815_WI_3_validation</td>\n",
       "      <td>0.170129</td>\n",
       "      <td>0.163820</td>\n",
       "      <td>0.142841</td>\n",
       "      <td>0.142171</td>\n",
       "      <td>0.175080</td>\n",
       "      <td>0.309772</td>\n",
       "      <td>0.314028</td>\n",
       "      <td>0.354919</td>\n",
       "      <td>0.349952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207519</td>\n",
       "      <td>0.288352</td>\n",
       "      <td>0.327491</td>\n",
       "      <td>0.216204</td>\n",
       "      <td>0.209463</td>\n",
       "      <td>0.186449</td>\n",
       "      <td>0.179426</td>\n",
       "      <td>0.171983</td>\n",
       "      <td>0.223477</td>\n",
       "      <td>0.228589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30478</th>\n",
       "      <td>FOODS_3_816_WI_3_validation</td>\n",
       "      <td>1.106222</td>\n",
       "      <td>1.662218</td>\n",
       "      <td>2.056831</td>\n",
       "      <td>2.182358</td>\n",
       "      <td>3.137803</td>\n",
       "      <td>3.945837</td>\n",
       "      <td>4.159928</td>\n",
       "      <td>5.639392</td>\n",
       "      <td>6.458342</td>\n",
       "      <td>...</td>\n",
       "      <td>4.901765</td>\n",
       "      <td>7.037045</td>\n",
       "      <td>7.132638</td>\n",
       "      <td>3.930789</td>\n",
       "      <td>3.743741</td>\n",
       "      <td>3.310497</td>\n",
       "      <td>3.103009</td>\n",
       "      <td>3.877436</td>\n",
       "      <td>4.334849</td>\n",
       "      <td>4.026009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30479</th>\n",
       "      <td>FOODS_3_817_WI_3_validation</td>\n",
       "      <td>0.248483</td>\n",
       "      <td>0.217399</td>\n",
       "      <td>0.222478</td>\n",
       "      <td>0.206802</td>\n",
       "      <td>0.252519</td>\n",
       "      <td>0.334509</td>\n",
       "      <td>0.245181</td>\n",
       "      <td>0.249568</td>\n",
       "      <td>0.269975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253558</td>\n",
       "      <td>0.322904</td>\n",
       "      <td>0.367291</td>\n",
       "      <td>0.256459</td>\n",
       "      <td>0.231512</td>\n",
       "      <td>0.201630</td>\n",
       "      <td>0.212455</td>\n",
       "      <td>0.190407</td>\n",
       "      <td>0.243462</td>\n",
       "      <td>0.272742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30480</th>\n",
       "      <td>FOODS_3_818_WI_3_validation</td>\n",
       "      <td>1.746840</td>\n",
       "      <td>1.728907</td>\n",
       "      <td>1.537579</td>\n",
       "      <td>1.486608</td>\n",
       "      <td>1.771760</td>\n",
       "      <td>1.925170</td>\n",
       "      <td>1.571907</td>\n",
       "      <td>2.077719</td>\n",
       "      <td>2.255561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347679</td>\n",
       "      <td>1.760127</td>\n",
       "      <td>2.113845</td>\n",
       "      <td>1.613122</td>\n",
       "      <td>1.496169</td>\n",
       "      <td>1.354901</td>\n",
       "      <td>1.422455</td>\n",
       "      <td>1.537526</td>\n",
       "      <td>1.637914</td>\n",
       "      <td>1.678456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30481</th>\n",
       "      <td>FOODS_3_819_WI_3_validation</td>\n",
       "      <td>1.721377</td>\n",
       "      <td>1.507604</td>\n",
       "      <td>1.483584</td>\n",
       "      <td>1.490049</td>\n",
       "      <td>1.983750</td>\n",
       "      <td>2.123987</td>\n",
       "      <td>1.954516</td>\n",
       "      <td>2.601582</td>\n",
       "      <td>2.782970</td>\n",
       "      <td>...</td>\n",
       "      <td>2.109782</td>\n",
       "      <td>2.748587</td>\n",
       "      <td>2.842023</td>\n",
       "      <td>2.228895</td>\n",
       "      <td>1.831670</td>\n",
       "      <td>1.730540</td>\n",
       "      <td>1.707281</td>\n",
       "      <td>1.940437</td>\n",
       "      <td>2.272692</td>\n",
       "      <td>2.293738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30482</th>\n",
       "      <td>FOODS_3_820_WI_3_validation</td>\n",
       "      <td>1.388087</td>\n",
       "      <td>1.336109</td>\n",
       "      <td>1.222412</td>\n",
       "      <td>1.260112</td>\n",
       "      <td>1.540383</td>\n",
       "      <td>1.871017</td>\n",
       "      <td>1.605662</td>\n",
       "      <td>1.867153</td>\n",
       "      <td>1.862762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.593405</td>\n",
       "      <td>1.961775</td>\n",
       "      <td>2.004126</td>\n",
       "      <td>1.526157</td>\n",
       "      <td>1.455012</td>\n",
       "      <td>1.394784</td>\n",
       "      <td>1.408600</td>\n",
       "      <td>1.607565</td>\n",
       "      <td>1.787998</td>\n",
       "      <td>1.678556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30483</th>\n",
       "      <td>FOODS_3_821_WI_3_validation</td>\n",
       "      <td>0.864728</td>\n",
       "      <td>0.834053</td>\n",
       "      <td>0.794988</td>\n",
       "      <td>0.803957</td>\n",
       "      <td>1.038775</td>\n",
       "      <td>1.356764</td>\n",
       "      <td>1.171732</td>\n",
       "      <td>1.098970</td>\n",
       "      <td>1.106632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.055960</td>\n",
       "      <td>1.589619</td>\n",
       "      <td>1.650366</td>\n",
       "      <td>0.915168</td>\n",
       "      <td>0.823664</td>\n",
       "      <td>0.780834</td>\n",
       "      <td>0.840679</td>\n",
       "      <td>1.036773</td>\n",
       "      <td>1.402671</td>\n",
       "      <td>1.379217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30484</th>\n",
       "      <td>FOODS_3_822_WI_3_validation</td>\n",
       "      <td>1.900163</td>\n",
       "      <td>1.666982</td>\n",
       "      <td>1.572376</td>\n",
       "      <td>1.530003</td>\n",
       "      <td>1.858630</td>\n",
       "      <td>2.613039</td>\n",
       "      <td>2.822656</td>\n",
       "      <td>2.981863</td>\n",
       "      <td>2.777522</td>\n",
       "      <td>...</td>\n",
       "      <td>2.448411</td>\n",
       "      <td>3.304192</td>\n",
       "      <td>3.662555</td>\n",
       "      <td>2.459101</td>\n",
       "      <td>2.122429</td>\n",
       "      <td>1.932835</td>\n",
       "      <td>1.801292</td>\n",
       "      <td>2.268423</td>\n",
       "      <td>2.748628</td>\n",
       "      <td>2.788297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_validation</td>\n",
       "      <td>0.424229</td>\n",
       "      <td>0.382899</td>\n",
       "      <td>0.374195</td>\n",
       "      <td>0.376270</td>\n",
       "      <td>0.427893</td>\n",
       "      <td>0.510383</td>\n",
       "      <td>0.462622</td>\n",
       "      <td>0.526742</td>\n",
       "      <td>0.530633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501694</td>\n",
       "      <td>0.574096</td>\n",
       "      <td>0.680916</td>\n",
       "      <td>0.502068</td>\n",
       "      <td>0.476109</td>\n",
       "      <td>0.451181</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.456869</td>\n",
       "      <td>0.477459</td>\n",
       "      <td>0.559477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_validation</td>\n",
       "      <td>0.278468</td>\n",
       "      <td>0.244065</td>\n",
       "      <td>0.249295</td>\n",
       "      <td>0.239328</td>\n",
       "      <td>0.267085</td>\n",
       "      <td>0.332627</td>\n",
       "      <td>0.309120</td>\n",
       "      <td>0.416650</td>\n",
       "      <td>0.419168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310013</td>\n",
       "      <td>0.435833</td>\n",
       "      <td>0.486012</td>\n",
       "      <td>0.373521</td>\n",
       "      <td>0.268836</td>\n",
       "      <td>0.240489</td>\n",
       "      <td>0.212437</td>\n",
       "      <td>0.233079</td>\n",
       "      <td>0.291724</td>\n",
       "      <td>0.291018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_validation</td>\n",
       "      <td>0.688014</td>\n",
       "      <td>0.541031</td>\n",
       "      <td>0.478711</td>\n",
       "      <td>0.519258</td>\n",
       "      <td>0.703169</td>\n",
       "      <td>0.872039</td>\n",
       "      <td>0.911149</td>\n",
       "      <td>1.173432</td>\n",
       "      <td>1.120225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936651</td>\n",
       "      <td>1.359380</td>\n",
       "      <td>1.528836</td>\n",
       "      <td>1.034646</td>\n",
       "      <td>0.746251</td>\n",
       "      <td>0.719357</td>\n",
       "      <td>0.603196</td>\n",
       "      <td>0.719309</td>\n",
       "      <td>0.870361</td>\n",
       "      <td>0.839945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_validation</td>\n",
       "      <td>0.984633</td>\n",
       "      <td>0.921479</td>\n",
       "      <td>0.797774</td>\n",
       "      <td>0.805375</td>\n",
       "      <td>1.011321</td>\n",
       "      <td>1.360545</td>\n",
       "      <td>1.196992</td>\n",
       "      <td>1.214167</td>\n",
       "      <td>1.265689</td>\n",
       "      <td>...</td>\n",
       "      <td>1.038729</td>\n",
       "      <td>1.446359</td>\n",
       "      <td>1.488667</td>\n",
       "      <td>0.990490</td>\n",
       "      <td>0.964610</td>\n",
       "      <td>0.894445</td>\n",
       "      <td>0.879212</td>\n",
       "      <td>0.956475</td>\n",
       "      <td>1.070195</td>\n",
       "      <td>1.237017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_validation</td>\n",
       "      <td>0.211334</td>\n",
       "      <td>1.058892</td>\n",
       "      <td>1.239285</td>\n",
       "      <td>1.880774</td>\n",
       "      <td>2.283138</td>\n",
       "      <td>3.238716</td>\n",
       "      <td>2.364665</td>\n",
       "      <td>1.805628</td>\n",
       "      <td>2.385788</td>\n",
       "      <td>...</td>\n",
       "      <td>2.012136</td>\n",
       "      <td>2.258794</td>\n",
       "      <td>2.414643</td>\n",
       "      <td>1.620300</td>\n",
       "      <td>1.708748</td>\n",
       "      <td>1.571339</td>\n",
       "      <td>1.524480</td>\n",
       "      <td>1.796847</td>\n",
       "      <td>2.351231</td>\n",
       "      <td>2.183592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id         F1         F2         F3  \\\n",
       "0      HOBBIES_1_001_CA_1_validation   0.817276   0.770136   0.773063   \n",
       "1      HOBBIES_1_002_CA_1_validation   0.161672   0.157221   0.150143   \n",
       "2      HOBBIES_1_003_CA_1_validation   0.431134   0.406738   0.424826   \n",
       "3      HOBBIES_1_004_CA_1_validation   1.677102   1.399713   1.320517   \n",
       "4      HOBBIES_1_005_CA_1_validation   0.926041   0.812809   0.782949   \n",
       "5      HOBBIES_1_006_CA_1_validation   0.804261   0.783431   0.774554   \n",
       "6      HOBBIES_1_007_CA_1_validation   0.296339   0.255662   0.265615   \n",
       "7      HOBBIES_1_008_CA_1_validation   6.330922   7.418705   8.388321   \n",
       "8      HOBBIES_1_009_CA_1_validation   0.842939   0.914360   0.793283   \n",
       "9      HOBBIES_1_010_CA_1_validation   0.600570   0.476667   0.493015   \n",
       "10     HOBBIES_1_011_CA_1_validation   0.074993   0.087449   0.086982   \n",
       "11     HOBBIES_1_012_CA_1_validation   0.181734   0.193036   0.179694   \n",
       "12     HOBBIES_1_013_CA_1_validation   0.312128   0.287521   0.279929   \n",
       "13     HOBBIES_1_014_CA_1_validation   1.680243   1.583166   1.530672   \n",
       "14     HOBBIES_1_015_CA_1_validation   3.196302   3.077121   3.251699   \n",
       "15     HOBBIES_1_016_CA_1_validation   5.766064   5.459336   5.048143   \n",
       "16     HOBBIES_1_017_CA_1_validation   0.945009   0.896739   0.830838   \n",
       "17     HOBBIES_1_018_CA_1_validation   0.012550   0.060461   0.057891   \n",
       "18     HOBBIES_1_019_CA_1_validation   7.361619   6.423784   6.686461   \n",
       "19     HOBBIES_1_020_CA_1_validation   0.261856   0.252665   0.247014   \n",
       "20     HOBBIES_1_021_CA_1_validation   0.580208   0.572484   0.571020   \n",
       "21     HOBBIES_1_022_CA_1_validation   0.356922   0.318589   0.275012   \n",
       "22     HOBBIES_1_023_CA_1_validation   1.418794   1.230790   1.146241   \n",
       "23     HOBBIES_1_024_CA_1_validation   0.222003   0.205209   0.196758   \n",
       "24     HOBBIES_1_025_CA_1_validation   0.483823   0.420764   0.446002   \n",
       "25     HOBBIES_1_026_CA_1_validation   0.191684   0.172332   0.177754   \n",
       "26     HOBBIES_1_027_CA_1_validation   0.351637   0.324101   0.321241   \n",
       "27     HOBBIES_1_028_CA_1_validation   0.561631   0.517261   0.536507   \n",
       "28     HOBBIES_1_029_CA_1_validation   1.592014   1.366301   1.299063   \n",
       "29     HOBBIES_1_030_CA_1_validation   5.292437   4.296557   4.136964   \n",
       "...                              ...        ...        ...        ...   \n",
       "30460    FOODS_3_798_WI_3_validation   0.321229   0.304133   0.293710   \n",
       "30461    FOODS_3_799_WI_3_validation   0.170079   0.178261   0.163283   \n",
       "30462    FOODS_3_800_WI_3_validation   6.282155   6.001415   5.909923   \n",
       "30463    FOODS_3_801_WI_3_validation   2.274663   2.107307   1.857913   \n",
       "30464    FOODS_3_802_WI_3_validation   0.231184   0.229869   0.222126   \n",
       "30465    FOODS_3_803_WI_3_validation   0.094647   0.797766   0.822766   \n",
       "30466    FOODS_3_804_WI_3_validation   4.411156   4.414146   4.288328   \n",
       "30467    FOODS_3_805_WI_3_validation   1.507966   1.391781   1.192361   \n",
       "30468    FOODS_3_806_WI_3_validation   0.563711   0.503447   0.432227   \n",
       "30469    FOODS_3_807_WI_3_validation   2.546803   2.316860   1.827463   \n",
       "30470    FOODS_3_808_WI_3_validation   0.200325   1.743984   3.175181   \n",
       "30471    FOODS_3_809_WI_3_validation   1.851644   1.760576   1.749796   \n",
       "30472    FOODS_3_810_WI_3_validation   0.631353   0.618864   0.490741   \n",
       "30473    FOODS_3_811_WI_3_validation  15.629869  14.836148  12.865305   \n",
       "30474    FOODS_3_812_WI_3_validation   1.034687   0.925409   0.942585   \n",
       "30475    FOODS_3_813_WI_3_validation   0.289267   0.271676   0.234282   \n",
       "30476    FOODS_3_814_WI_3_validation   1.741611   1.713006   1.713718   \n",
       "30477    FOODS_3_815_WI_3_validation   0.170129   0.163820   0.142841   \n",
       "30478    FOODS_3_816_WI_3_validation   1.106222   1.662218   2.056831   \n",
       "30479    FOODS_3_817_WI_3_validation   0.248483   0.217399   0.222478   \n",
       "30480    FOODS_3_818_WI_3_validation   1.746840   1.728907   1.537579   \n",
       "30481    FOODS_3_819_WI_3_validation   1.721377   1.507604   1.483584   \n",
       "30482    FOODS_3_820_WI_3_validation   1.388087   1.336109   1.222412   \n",
       "30483    FOODS_3_821_WI_3_validation   0.864728   0.834053   0.794988   \n",
       "30484    FOODS_3_822_WI_3_validation   1.900163   1.666982   1.572376   \n",
       "30485    FOODS_3_823_WI_3_validation   0.424229   0.382899   0.374195   \n",
       "30486    FOODS_3_824_WI_3_validation   0.278468   0.244065   0.249295   \n",
       "30487    FOODS_3_825_WI_3_validation   0.688014   0.541031   0.478711   \n",
       "30488    FOODS_3_826_WI_3_validation   0.984633   0.921479   0.797774   \n",
       "30489    FOODS_3_827_WI_3_validation   0.211334   1.058892   1.239285   \n",
       "\n",
       "              F4         F5         F6         F7         F8         F9  ...  \\\n",
       "0       0.734750   0.974026   1.159193   1.223346   0.918734   0.969244  ...   \n",
       "1       0.152620   0.200712   0.261922   0.289153   0.209044   0.189601  ...   \n",
       "2       0.435743   0.651485   0.875942   0.672834   0.480412   0.487221  ...   \n",
       "3       1.471098   1.804805   3.016687   2.998087   1.693713   1.477424  ...   \n",
       "4       0.850560   1.026819   1.428682   1.411533   0.992563   1.041681  ...   \n",
       "5       0.824596   0.855111   0.947720   1.094119   0.888501   0.815705  ...   \n",
       "6       0.272955   0.296285   0.348281   0.419274   0.314400   0.307129  ...   \n",
       "7       7.513185   8.782354   9.019364   7.580533   8.519221   7.916029  ...   \n",
       "8       0.892036   0.997161   1.108806   1.337384   0.955499   0.907876  ...   \n",
       "9       0.470509   0.573394   0.832895   0.915917   0.640671   0.665931  ...   \n",
       "10      0.092422   0.110987   0.134317   0.134498   0.082918   0.079890  ...   \n",
       "11      0.199297   0.221838   0.324098   0.309741   0.198635   0.186451  ...   \n",
       "12      0.302888   0.394982   0.508918   0.515574   0.321108   0.311120  ...   \n",
       "13      1.526282   1.733708   2.194308   2.212691   1.804338   1.647237  ...   \n",
       "14      3.227031   3.771418   5.000935   4.747428   3.339115   3.069590  ...   \n",
       "15      5.135166   5.468039   8.005105   6.202049   5.693248   5.160178  ...   \n",
       "16      0.881532   1.195535   1.659318   1.842948   1.051945   0.973063  ...   \n",
       "17      0.058446   0.072867   0.093913   0.094260   0.072050   0.066704  ...   \n",
       "18      7.105886   8.397991   8.518142   7.844825   8.096612   8.001851  ...   \n",
       "19      0.256279   0.276505   0.316110   0.313629   0.245563   0.230483  ...   \n",
       "20      0.655592   0.727170   0.884795   0.827396   0.605667   0.567853  ...   \n",
       "21      0.294085   0.418820   0.475940   0.528208   0.317346   0.299075  ...   \n",
       "22      1.258297   1.597155   2.038093   2.113230   1.560609   1.330694  ...   \n",
       "23      0.180647   0.249229   0.305855   0.287218   0.231260   0.213429  ...   \n",
       "24      0.498881   0.677113   0.854955   1.003496   0.467169   0.471854  ...   \n",
       "25      0.202233   0.225490   0.272854   0.322485   0.217753   0.199495  ...   \n",
       "26      0.314376   0.404987   0.641048   0.646389   0.361954   0.351747  ...   \n",
       "27      0.595838   0.713343   0.880129   0.969188   0.755633   0.710308  ...   \n",
       "28      1.352318   1.620083   2.093550   2.096189   1.491071   1.453033  ...   \n",
       "29      4.139414   6.221961   7.883964   7.882635   5.616298   5.560988  ...   \n",
       "...          ...        ...        ...        ...        ...        ...  ...   \n",
       "30460   0.286341   0.394677   0.505779   0.390127   0.406330   0.389569  ...   \n",
       "30461   0.178484   0.206090   0.262792   0.236955   0.279629   0.302050  ...   \n",
       "30462   6.044600   7.848129  10.047003   9.541103  10.591332  10.293884  ...   \n",
       "30463   1.979711   2.398504   3.319686   2.818320   3.292629   3.500330  ...   \n",
       "30464   0.263466   0.384435   0.496687   0.432486   0.442373   0.488369  ...   \n",
       "30465   1.295276   1.515854   1.553098   1.229574   1.209004   1.298607  ...   \n",
       "30466   3.976432   5.076610   6.495052   6.284872   7.541868   7.617856  ...   \n",
       "30467   1.335825   1.633870   2.131017   2.020013   2.185843   2.348764  ...   \n",
       "30468   0.432874   0.515128   0.598674   0.660179   0.669177   0.650455  ...   \n",
       "30469   1.955103   2.535162   3.058748   2.764864   3.554619   3.881389  ...   \n",
       "30470   5.354431   8.096684  14.209224  19.560379  21.738900  18.476413  ...   \n",
       "30471   1.792445   2.400186   2.514697   2.510362   3.382664   3.231788  ...   \n",
       "30472   0.502486   0.667869   0.814034   0.670948   0.648460   0.643499  ...   \n",
       "30473  12.714345  14.129779  18.429221  18.017380  20.270525  20.240907  ...   \n",
       "30474   0.944759   1.263260   1.401484   1.456685   1.826407   1.740029  ...   \n",
       "30475   0.229281   0.202783   0.247041   0.211932   0.338137   0.358633  ...   \n",
       "30476   1.778721   2.174868   2.412760   2.265626   2.407542   2.371163  ...   \n",
       "30477   0.142171   0.175080   0.309772   0.314028   0.354919   0.349952  ...   \n",
       "30478   2.182358   3.137803   3.945837   4.159928   5.639392   6.458342  ...   \n",
       "30479   0.206802   0.252519   0.334509   0.245181   0.249568   0.269975  ...   \n",
       "30480   1.486608   1.771760   1.925170   1.571907   2.077719   2.255561  ...   \n",
       "30481   1.490049   1.983750   2.123987   1.954516   2.601582   2.782970  ...   \n",
       "30482   1.260112   1.540383   1.871017   1.605662   1.867153   1.862762  ...   \n",
       "30483   0.803957   1.038775   1.356764   1.171732   1.098970   1.106632  ...   \n",
       "30484   1.530003   1.858630   2.613039   2.822656   2.981863   2.777522  ...   \n",
       "30485   0.376270   0.427893   0.510383   0.462622   0.526742   0.530633  ...   \n",
       "30486   0.239328   0.267085   0.332627   0.309120   0.416650   0.419168  ...   \n",
       "30487   0.519258   0.703169   0.872039   0.911149   1.173432   1.120225  ...   \n",
       "30488   0.805375   1.011321   1.360545   1.196992   1.214167   1.265689  ...   \n",
       "30489   1.880774   2.283138   3.238716   2.364665   1.805628   2.385788  ...   \n",
       "\n",
       "             F19        F20        F21        F22        F23        F24  \\\n",
       "0       0.779733   1.119184   1.027063   0.828853   0.806498   0.808823   \n",
       "1       0.197146   0.210441   0.214696   0.174457   0.162774   0.171702   \n",
       "2       0.509719   0.642228   0.673082   0.491763   0.423794   0.430336   \n",
       "3       1.816084   2.601878   3.209413   1.592659   1.483907   1.377427   \n",
       "4       1.042151   1.459160   1.587306   0.987382   0.855638   0.884560   \n",
       "5       0.686038   0.934931   0.931819   0.803367   0.686886   0.749135   \n",
       "6       0.333472   0.394438   0.474240   0.331565   0.349792   0.320193   \n",
       "7       8.001297   9.079751   8.545755   7.654284   7.249439   7.254823   \n",
       "8       0.888622   1.143173   1.278639   0.941930   0.985323   0.851697   \n",
       "9       0.576035   0.717115   0.805349   0.592424   0.516743   0.528799   \n",
       "10      0.114705   0.168386   0.175080   0.096131   0.097127   0.109733   \n",
       "11      0.191741   0.311125   0.400580   0.210034   0.179597   0.187246   \n",
       "12      0.331940   0.572990   0.595005   0.390757   0.321206   0.340255   \n",
       "13      1.594097   1.832842   1.843334   1.540705   1.404503   1.569764   \n",
       "14      3.476131   4.986322   4.489625   2.721986   2.780687   2.698921   \n",
       "15      5.775569   7.974805   7.422209   5.467073   4.825719   5.435774   \n",
       "16      1.246429   1.519458   1.665724   0.942445   0.937349   0.989703   \n",
       "17      0.093462   0.111386   0.114844   0.087953   0.081231   0.085177   \n",
       "18      7.447989   8.489655   7.651754   6.689675   5.948963   6.589930   \n",
       "19      0.270053   0.300276   0.304994   0.257892   0.263702   0.253771   \n",
       "20      0.817640   0.866398   0.890955   0.660190   0.631043   0.642046   \n",
       "21      0.310080   0.525231   0.556916   0.348454   0.303816   0.290070   \n",
       "22      1.630253   2.187635   2.182459   1.581973   1.193530   1.185981   \n",
       "23      0.233391   0.317032   0.339943   0.230085   0.202754   0.202407   \n",
       "24      0.636575   0.792754   0.995948   0.494750   0.443552   0.429469   \n",
       "25      0.193005   0.246253   0.253732   0.194431   0.184959   0.171535   \n",
       "26      0.411727   0.546008   0.561487   0.337373   0.350231   0.308079   \n",
       "27      0.786059   0.817497   0.812331   0.676202   0.574158   0.596210   \n",
       "28      1.741985   2.187173   2.212590   1.533194   1.406845   1.277220   \n",
       "29      6.563665   9.084987   8.476809   5.431188   4.647069   4.935142   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "30460   0.451224   0.550685   0.567712   0.383665   0.355652   0.339711   \n",
       "30461   0.296256   0.343787   0.316611   0.268926   0.275248   0.263621   \n",
       "30462   8.918664  11.824297  12.799487   8.290161   7.288364   6.714998   \n",
       "30463   2.873259   3.705753   3.671871   2.524961   2.316742   2.257988   \n",
       "30464   0.505850   0.610420   0.710522   0.386773   0.329013   0.304977   \n",
       "30465   1.331270   1.607535   1.546699   1.079257   1.089646   1.030142   \n",
       "30466   6.931394   9.473998   8.717522   6.552831   5.874569   5.816035   \n",
       "30467   1.701333   2.518507   2.317897   1.729988   1.552497   1.454900   \n",
       "30468   0.600637   0.832547   0.862744   0.566447   0.552940   0.565051   \n",
       "30469   2.955379   4.169486   4.374120   3.313321   2.879224   2.566886   \n",
       "30470  14.194121  19.526597  21.260601  15.684528  13.047093  10.947351   \n",
       "30471   2.372498   2.983672   2.765355   2.482787   2.084702   1.829149   \n",
       "30472   0.660334   0.756267   0.805342   0.641139   0.603470   0.597127   \n",
       "30473  16.130130  22.199482  22.514359  14.233854  13.936478  12.075680   \n",
       "30474   1.511094   1.858865   2.078866   1.176930   1.036719   1.075472   \n",
       "30475   0.267152   0.343628   0.373080   0.355992   0.334263   0.315004   \n",
       "30476   2.206821   2.749198   2.806340   1.978183   1.752176   1.724408   \n",
       "30477   0.207519   0.288352   0.327491   0.216204   0.209463   0.186449   \n",
       "30478   4.901765   7.037045   7.132638   3.930789   3.743741   3.310497   \n",
       "30479   0.253558   0.322904   0.367291   0.256459   0.231512   0.201630   \n",
       "30480   1.347679   1.760127   2.113845   1.613122   1.496169   1.354901   \n",
       "30481   2.109782   2.748587   2.842023   2.228895   1.831670   1.730540   \n",
       "30482   1.593405   1.961775   2.004126   1.526157   1.455012   1.394784   \n",
       "30483   1.055960   1.589619   1.650366   0.915168   0.823664   0.780834   \n",
       "30484   2.448411   3.304192   3.662555   2.459101   2.122429   1.932835   \n",
       "30485   0.501694   0.574096   0.680916   0.502068   0.476109   0.451181   \n",
       "30486   0.310013   0.435833   0.486012   0.373521   0.268836   0.240489   \n",
       "30487   0.936651   1.359380   1.528836   1.034646   0.746251   0.719357   \n",
       "30488   1.038729   1.446359   1.488667   0.990490   0.964610   0.894445   \n",
       "30489   2.012136   2.258794   2.414643   1.620300   1.708748   1.571339   \n",
       "\n",
       "             F25        F26        F27        F28  \n",
       "0       0.790493   0.935414   1.129197   1.058533  \n",
       "1       0.177484   0.178244   0.250040   0.239309  \n",
       "2       0.463548   0.586319   0.726258   0.719822  \n",
       "3       1.345501   1.821208   2.918769   3.293197  \n",
       "4       0.859523   1.088437   1.383876   1.465852  \n",
       "5       0.756473   0.733492   0.863749   0.850394  \n",
       "6       0.307124   0.313301   0.412904   0.414125  \n",
       "7       8.063553   7.566560   8.891785   6.633204  \n",
       "8       0.859277   0.853214   1.136018   1.139906  \n",
       "9       0.566638   0.657165   0.763480   0.838685  \n",
       "10      0.124072   0.143694   0.163460   0.171485  \n",
       "11      0.217563   0.251681   0.359535   0.371984  \n",
       "12      0.296404   0.391233   0.535591   0.591458  \n",
       "13      1.655459   1.711671   1.929680   1.773803  \n",
       "14      2.988750   3.733684   5.115591   4.831274  \n",
       "15      5.282380   6.168832   7.690149   6.848590  \n",
       "16      0.966366   1.342874   1.906462   1.798525  \n",
       "17      0.086814   0.110446   0.127377   0.131617  \n",
       "18      7.367831   8.445051   8.304362   8.762527  \n",
       "19      0.259087   0.296674   0.375430   0.390166  \n",
       "20      0.630578   0.737626   0.973417   0.961644  \n",
       "21      0.279553   0.346886   0.487306   0.508503  \n",
       "22      1.189071   1.613514   2.247573   2.139162  \n",
       "23      0.192086   0.275916   0.344100   0.344243  \n",
       "24      0.424702   0.605102   0.796178   0.862890  \n",
       "25      0.170053   0.214665   0.264125   0.268291  \n",
       "26      0.320112   0.396613   0.510721   0.513784  \n",
       "27      0.676081   0.744756   0.907394   0.801126  \n",
       "28      1.243139   1.707174   1.945735   2.089233  \n",
       "29      4.568084   6.173704   8.576298   8.214426  \n",
       "...          ...        ...        ...        ...  \n",
       "30460   0.330426   0.448856   0.532729   0.498438  \n",
       "30461   0.211299   0.203877   0.264354   0.254505  \n",
       "30462   7.096859   8.260453   9.861054   9.713196  \n",
       "30463   2.257700   2.801325   2.734822   2.745616  \n",
       "30464   0.273506   0.375846   0.464383   0.441479  \n",
       "30465   1.032716   1.227434   1.432852   1.304518  \n",
       "30466   6.077403   6.990247   7.856869   7.030278  \n",
       "30467   1.449240   1.712414   1.855091   1.782754  \n",
       "30468   0.517177   0.533310   0.662704   0.617546  \n",
       "30469   2.612659   2.707073   3.277590   3.267757  \n",
       "30470  10.220070  11.146726  14.446118  14.842901  \n",
       "30471   1.901561   2.303145   2.321571   1.953122  \n",
       "30472   0.554197   0.639804   0.782455   0.770890  \n",
       "30473  13.063460  13.865376  15.717298  14.278867  \n",
       "30474   1.013438   1.176367   1.416324   1.449187  \n",
       "30475   0.286013   0.274436   0.300007   0.289749  \n",
       "30476   1.711915   2.020434   2.394021   2.241675  \n",
       "30477   0.179426   0.171983   0.223477   0.228589  \n",
       "30478   3.103009   3.877436   4.334849   4.026009  \n",
       "30479   0.212455   0.190407   0.243462   0.272742  \n",
       "30480   1.422455   1.537526   1.637914   1.678456  \n",
       "30481   1.707281   1.940437   2.272692   2.293738  \n",
       "30482   1.408600   1.607565   1.787998   1.678556  \n",
       "30483   0.840679   1.036773   1.402671   1.379217  \n",
       "30484   1.801292   2.268423   2.748628   2.788297  \n",
       "30485   0.409836   0.456869   0.477459   0.559477  \n",
       "30486   0.212437   0.233079   0.291724   0.291018  \n",
       "30487   0.603196   0.719309   0.870361   0.839945  \n",
       "30488   0.879212   0.956475   1.070195   1.237017  \n",
       "30489   1.524480   1.796847   2.351231   2.183592  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n",
    "        if USE_AUX:\n",
    "            model_path = AUX_MODELS + model_path\n",
    "        \n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask)&(store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv.gz')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('submission_v'+str(VER)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "# Of course here is no magic at all.\n",
    "# No \"Novel\" features and no brilliant ideas.\n",
    "# We just carefully joined all\n",
    "# our previous fe work and created a model.\n",
    "\n",
    "# Also!\n",
    "# In my opinion this strategy is a \"dead end\".\n",
    "# Overfits a lot LB and with 1 final submission \n",
    "# you have no option to risk.\n",
    "\n",
    "\n",
    "# Improvement should come from:\n",
    "# Loss function\n",
    "# Data representation\n",
    "# Stable CV\n",
    "# Good features reduction strategy\n",
    "# Predictions stabilization with NN\n",
    "# Trend prediction\n",
    "# Real zero sales detection/classification\n",
    "\n",
    "\n",
    "# Good kernels references \n",
    "## (the order is random and the list is not complete):\n",
    "# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
    "# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
    "# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
    "# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n",
    "# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n",
    "# https://www.kaggle.com/yassinealouini/seq2seq\n",
    "# https://www.kaggle.com/kailex/m5-forecaster-v2\n",
    "# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n",
    "\n",
    "\n",
    "# Features were created in these kernels:\n",
    "## \n",
    "# Mean encodings and PCA options\n",
    "# https://www.kaggle.com/kyakovlev/m5-custom-features\n",
    "##\n",
    "# Lags and rolling lags\n",
    "# https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "##\n",
    "# Base Grid and base features (calendar/price/etc)\n",
    "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
    "\n",
    "\n",
    "# Personal request\n",
    "# Please don't upvote any ensemble and copypaste kernels\n",
    "## The worst case is ensemble without any analyse.\n",
    "## The best choice - just ignore it.\n",
    "## I would like to see more kernels with interesting and original approaches.\n",
    "## Don't feed copypasters with upvotes.\n",
    "\n",
    "## It doesn't mean that you should not fork and improve others kernels\n",
    "## but I would like to see params and code tuning based on some CV and analyse\n",
    "## and not only on LB probing.\n",
    "## Small changes could be shared in comments and authors can improve their kernel.\n",
    "\n",
    "## Feel free to criticize this kernel as my knowlege is very limited\n",
    "## and I can be wrong in code and descriptions. \n",
    "## Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
