{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## メモリ使用量を確認するためのシンプルな「メモリプロファイラ」\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory Reducer\n",
    "# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n",
    "# :verbose                                        # type: bool\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging by concat to not lose dtypes\n",
    "def merge_by_concat(df1, df2, merge_on):\n",
    "    merged_gf = df1[merge_on]\n",
    "    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
    "    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
    "    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1913         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Main Data\n"
     ]
    }
   ],
   "source": [
    "########################### Load Data\n",
    "#################################################################################\n",
    "print('Load Main Data')\n",
    "\n",
    "# Here are reafing all our data \n",
    "# without any limitations and dtype modification\n",
    "train_df = pd.read_csv('../input/csv/sales_train_validation.csv.gz')\n",
    "prices_df = pd.read_csv('../input/csv/sell_prices.csv.gz')\n",
    "calendar_df = pd.read_csv('../input/csv/calendar.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Grid\n",
      "Train rows: 30490 58327370\n",
      "    Original grid_df:   3.5GiB\n",
      "     Reduced grid_df:   1.3GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Make Grid\n",
    "#################################################################################\n",
    "print('Create Grid')\n",
    "\n",
    "# We can tranform horizontal representation \n",
    "# to vertical \"view\"\n",
    "# Our \"index\" will be 'id','item_id','dept_id','cat_id','store_id','state_id'\n",
    "# and labels are 'd_' coulmns\n",
    "\n",
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "grid_df = pd.melt(train_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "\n",
    "# If we look on train_df we se that \n",
    "# we don't have a lot of traning rows\n",
    "# but each day can provide more train data\n",
    "print('Train rows:', len(train_df), len(grid_df))\n",
    "\n",
    "# To be able to make predictions\n",
    "# we need to add \"test set\" to our grid\n",
    "add_grid = pd.DataFrame()\n",
    "for i in range(1,29):\n",
    "    temp_df = train_df[index_columns]\n",
    "    temp_df = temp_df.drop_duplicates()\n",
    "    temp_df['d'] = 'd_'+ str(END_TRAIN+i)\n",
    "    temp_df[TARGET] = np.nan\n",
    "    add_grid = pd.concat([add_grid,temp_df])\n",
    "\n",
    "grid_df = pd.concat([grid_df,add_grid])\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Remove some temoprary DFs\n",
    "del temp_df, add_grid\n",
    "\n",
    "# We will not need original train_df\n",
    "# anymore and can remove it\n",
    "del train_df\n",
    "\n",
    "# You don't have to use df = df construction\n",
    "# you can use inplace=True instead.\n",
    "# like this\n",
    "# grid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# We can free some memory \n",
    "# by converting \"strings\" to categorical\n",
    "# it will not affect merging and \n",
    "# we will not lose any valuable data\n",
    "for col in index_columns:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release week\n",
      "    Original grid_df:   1.7GiB\n",
      "     Reduced grid_df:   1.5GiB\n"
     ]
    }
   ],
   "source": [
    "########################### Product Release date\n",
    "#################################################################################\n",
    "print('Release week')\n",
    "\n",
    "# It seems that leadings zero values\n",
    "# in each train_df item row\n",
    "# are not real 0 sales but mean\n",
    "# absence for the item in the store\n",
    "# we can safe some memory by removing\n",
    "# such zeros\n",
    "\n",
    "# Prices are set by week\n",
    "# so it we will have not very accurate release week \n",
    "release_df = prices_df.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "release_df.columns = ['store_id','item_id','release']\n",
    "\n",
    "# Now we can merge release_df\n",
    "grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])\n",
    "del release_df\n",
    "\n",
    "# We want to remove some \"zeros\" rows\n",
    "# from grid_df \n",
    "# to do it we need wm_yr_wk column\n",
    "# let's merge partly calendar_df to have it\n",
    "grid_df = merge_by_concat(grid_df, calendar_df[['wm_yr_wk','d']], ['d'])\n",
    "                      \n",
    "# Now we can cutoff some rows \n",
    "# and safe memory \n",
    "grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]\n",
    "grid_df = grid_df.reset_index(drop=True)\n",
    "\n",
    "# Let's check our memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "\n",
    "# Should we keep release week \n",
    "# as one of the features?\n",
    "# Only good CV can give the answer.\n",
    "# Let's minify the release values.\n",
    "# Min transformation will not help here \n",
    "# as int16 -> Integer (-32768 to 32767)\n",
    "# and our grid_df['release'].max() serves for int16\n",
    "# but we have have an idea how to transform \n",
    "# other columns in case we will need it\n",
    "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
    "grid_df['release'] = grid_df['release'].astype(np.int16)\n",
    "\n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Part 1\n",
      "Size: (46881677, 10)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 1\n",
    "#################################################################################\n",
    "print('Save Part 1')\n",
    "\n",
    "# We have our BASE grid ready\n",
    "# and can save it as pickle file\n",
    "# for future use (model training)\n",
    "grid_df.to_pickle('grid_part_1.pkl.gz')\n",
    "\n",
    "print('Size:', grid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices\n"
     ]
    }
   ],
   "source": [
    "########################### Prices\n",
    "#################################################################################\n",
    "print('Prices')\n",
    "\n",
    "# We can do some basic aggregations\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge prices and save part 2\n",
      "Mem. usage decreased to 1789.88 Mb (62.2% reduction)\n",
      "Size: (46881677, 13)\n"
     ]
    }
   ],
   "source": [
    "########################### Merge prices and save part 2\n",
    "#################################################################################\n",
    "print('Merge prices and save part 2')\n",
    "\n",
    "# Merge Prices\n",
    "original_columns = list(grid_df)\n",
    "grid_df = grid_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "keep_columns = [col for col in list(grid_df) if col not in original_columns]\n",
    "grid_df = grid_df[MAIN_INDEX+keep_columns]\n",
    "grid_df = reduce_mem_usage(grid_df)\n",
    "\n",
    "# Safe part 2\n",
    "grid_df.to_pickle('grid_part_2.pkl.gz')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need prices_df anymore\n",
    "del prices_df\n",
    "\n",
    "# We can remove new columns\n",
    "# or just load part_1\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Merge calendar\n",
    "#################################################################################\n",
    "grid_df = grid_df[MAIN_INDEX]\n",
    "\n",
    "# Merge calendar partly\n",
    "icols = ['date',\n",
    "         'd',\n",
    "         'event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "\n",
    "grid_df = grid_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "\n",
    "# Minify data\n",
    "# 'snap_' columns we can convert to bool or int8\n",
    "icols = ['event_name_1',\n",
    "         'event_type_1',\n",
    "         'event_name_2',\n",
    "         'event_type_2',\n",
    "         'snap_CA',\n",
    "         'snap_TX',\n",
    "         'snap_WI']\n",
    "for col in icols:\n",
    "    grid_df[col] = grid_df[col].astype('category')\n",
    "\n",
    "# Convert to DateTime\n",
    "grid_df['date'] = pd.to_datetime(grid_df['date'])\n",
    "\n",
    "# Make some features from date\n",
    "grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)\n",
    "grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)\n",
    "grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)\n",
    "grid_df['tm_y'] = grid_df['date'].dt.year\n",
    "grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)\n",
    "grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "\n",
    "grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)\n",
    "grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "# Remove date\n",
    "del grid_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save part 3\n",
      "Size: (46881677, 16)\n"
     ]
    }
   ],
   "source": [
    "########################### Save part 3 (Dates)\n",
    "#################################################################################\n",
    "print('Save part 3')\n",
    "\n",
    "# Safe part 3\n",
    "grid_df.to_pickle('grid_part_3.pkl.gz')\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# We don't need calendar_df anymore\n",
    "del calendar_df\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Some additional cleaning\n",
    "#################################################################################\n",
    "\n",
    "## Part 1\n",
    "# Convert 'd' to int\n",
    "grid_df = pd.read_pickle('grid_part_1.pkl.gz')\n",
    "grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)\n",
    "\n",
    "# Remove 'wm_yr_wk'\n",
    "# as test values are not in train set\n",
    "del grid_df['wm_yr_wk']\n",
    "grid_df.to_pickle('grid_part_1.pkl.gz')\n",
    "\n",
    "del grid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Full Grid:   2.4GiB\n",
      "Size: (46881677, 34)\n",
      "           Full Grid:   1.1GiB\n",
      "           Full Grid: 293.8MiB\n"
     ]
    }
   ],
   "source": [
    "########################### Summary\n",
    "#################################################################################\n",
    "\n",
    "# Now we have 3 sets of features\n",
    "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl.gz'),\n",
    "                     pd.read_pickle('grid_part_2.pkl.gz').iloc[:,2:],\n",
    "                     pd.read_pickle('grid_part_3.pkl.gz').iloc[:,2:]],\n",
    "                     axis=1)\n",
    "                     \n",
    "# Let's check again memory usage\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "print('Size:', grid_df.shape)\n",
    "\n",
    "# 2.5GiB + is is still too big to train our model\n",
    "# (on kaggle with its memory limits)\n",
    "# and we don't have lag features yet\n",
    "# But what if we can train by state_id or shop_id?\n",
    "state_id = 'CA'\n",
    "grid_df = grid_df[grid_df['state_id']==state_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid:   1.2GiB\n",
    "\n",
    "store_id = 'CA_1'\n",
    "grid_df = grid_df[grid_df['store_id']==store_id]\n",
    "print(\"{:>20}: {:>8}\".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))\n",
    "#           Full Grid: 321.2MiB\n",
    "\n",
    "# Seems its good enough now\n",
    "# In other kernel we will talk about LAGS features\n",
    "# Thank you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4788267 entries, 0 to 46854235\n",
      "Data columns (total 34 columns):\n",
      " #   Column            Dtype   \n",
      "---  ------            -----   \n",
      " 0   id                category\n",
      " 1   item_id           category\n",
      " 2   dept_id           category\n",
      " 3   cat_id            category\n",
      " 4   store_id          category\n",
      " 5   state_id          category\n",
      " 6   d                 int16   \n",
      " 7   sales             float64 \n",
      " 8   release           int16   \n",
      " 9   sell_price        float16 \n",
      " 10  price_max         float16 \n",
      " 11  price_min         float16 \n",
      " 12  price_std         float16 \n",
      " 13  price_mean        float16 \n",
      " 14  price_norm        float16 \n",
      " 15  price_nunique     float16 \n",
      " 16  item_nunique      int16   \n",
      " 17  price_momentum    float16 \n",
      " 18  price_momentum_m  float16 \n",
      " 19  price_momentum_y  float16 \n",
      " 20  event_name_1      category\n",
      " 21  event_type_1      category\n",
      " 22  event_name_2      category\n",
      " 23  event_type_2      category\n",
      " 24  snap_CA           category\n",
      " 25  snap_TX           category\n",
      " 26  snap_WI           category\n",
      " 27  tm_d              int8    \n",
      " 28  tm_w              int8    \n",
      " 29  tm_m              int8    \n",
      " 30  tm_y              int8    \n",
      " 31  tm_wm             int8    \n",
      " 32  tm_dw             int8    \n",
      " 33  tm_w_end          int8    \n",
      "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
      "memory usage: 293.8 MB\n"
     ]
    }
   ],
   "source": [
    "########################### Final list of features\n",
    "#################################################################################\n",
    "grid_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
